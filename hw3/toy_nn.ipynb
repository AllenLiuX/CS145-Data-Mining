{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 3, Part 2: Neural Networks\n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW3 is due on **11:59 PM PT, Nov 9 (Monday, Week 6)**. Please submit through GradeScope. \n",
    "\n",
    "Note that, Howework #3 has two jupyter notebooks to complete (Part 1: kNN and Part 2: Neural Network).  \n",
    "\n",
    "----\n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Wenxuan Liu, UID: 805152602** </span>\n",
    "\n",
    "----\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW3 conda environment by the given `cs145hw3.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw3.yml\n",
    "conda activate hw3\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw3.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks (such as hyperparameters) that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Backprop in a neural network \n",
    "\n",
    "<span style=\"color:red\"> Note: Section 1 is \"question-answer\" style problem. You do not need to code anything and you are required to calculate by hand (with a scientific calculator), which helps you understand the back propagation in neural networks. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, let's consider a simple two-layer neural network and manually do the forward and backward pass. For simplicity, we assume our input data is two dimension. Then the model architecture looks like the following. Notice that in the example we saw in class, the bias term `b` was not explicit listed in the architecture diagram. Here we include the term `b` explicitly for each layer in the diagram. Recall the formula for computing $\\mathbf{x^{(l)}}$ in the $l$-th layer from $\\mathbf{x^{(l-1)}}$ in the $(l-1)$-th layer is $\\mathbf{x^{(l)}} = \\mathbf{f^{(l)}(W^{(l)} x^{(l-1)} + b^{(l)})}$. The activation function $\\mathbf{f^{(l)}}$ we choose is the `sigmoid` function for all layers, i.e. $\\mathbf{f^{(l)}}(z) = \\frac{1}{1+\\exp(-z)}$. The final loss function is $\\frac{1}{2}$ of the `mean squared error` loss, i.e. $l\\mathbf{(y, \\hat y)} = \\frac{1}{2} ||\\mathbf{y - \\hat y}||^2$. <img src=\"nn.png\"  width=\"350\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our weights as $$\\mathbf{W^{(1)}} = \\begin{bmatrix}\n",
    "0.15 & 0.2 \\\\\n",
    "0.25 & 0.3 \n",
    "\\end{bmatrix}, \\quad \\mathbf{W^{(2)}} = [0.4, 0.45], \\quad \\mathbf{b^{(1)}} = [0.35, 0.35], \\quad \\mathbf{b^{(2)}} = 0.6$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "**Questions**\n",
    "\n",
    "1. When the input $\\mathbf{x^{(0)}} = [0.05, 0.1]$, what will be the value of $\\mathbf{x^{(1)}}$ in the hidden layer? (Show your work).\n",
    "2. Based on the value $\\mathbf{x^{(1)}}$ you computed, what will be the value of $\\mathbf{x^{(2)}}$ in the output layer? (Show your work).\n",
    "3. When the target value of this input is $y = 0.01$, based on the value $\\mathbf{x^{(2)}}$ you computed, what will be the loss? (Show your work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>\n",
    "1. \n",
    "\n",
    "\n",
    "$ x^{(1)} = \\mathbf{f^{(1)}(W^{(1)} x^{(0)} + b^{(1)})}$\n",
    "\n",
    "$= f^{(1)}(\\begin{bmatrix}\n",
    "0.15 & 0.2 \\\\\n",
    "0.25 & 0.3 \n",
    "\\end{bmatrix}*[0.05, 0.1]^T+[0.35, 0.35]) $\n",
    "\n",
    "$= f^{(1)}([0.3775, 0.3925])$\n",
    "\n",
    "$=[0.59326, 0.59688]$\n",
    "\n",
    "2. \n",
    "\n",
    "\n",
    "$ x^{(2)} = \\mathbf{f^{(2)}(W^{(2)} x^{(1)} + b^{(2)})} $\n",
    "\n",
    "$= f^{(2)}([0.59326, 0.59688][0.4, 0.45]^T+0.6)$\n",
    "\n",
    "$=f^{(2)}(1.1059) = 0.7514 $\n",
    "\n",
    "3. \n",
    "\n",
    "\n",
    "$ loss = l\\mathbf{(y, x^{(2)})} $\n",
    "\n",
    "$= \\frac{1}{2}||\\mathbf{y - x^{(2)}}||^2 $\n",
    "\n",
    "$= \\frac12*||0.01-0.7514||^2=0.2748$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "With the loss computed below, we are ready for a backward pass to update the weights in the neural network. Kindly remind that the gradients of a variable should have the same shape with the variable.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "1. Consider the loss $l$ of the same input $\\mathbf{x^{(0)}} = [0.05, 0.1]$, what will be the update of $\\mathbf{W^{(2)}}$ and $\\mathbf{b^{(2)}}$ when we backprop, i.e. $\\frac{\\partial l}{\\partial \\mathbf{W^{(2)}}}$, $\\frac{\\partial l}{\\partial \\mathbf{b^{(2)}}}$  (Show your work in detailed calculation steps. Answers without justification will not be credited.).\n",
    "2. Based on the result you computed in part 1, when we keep backproping, what will be the update of $\\mathbf{W^{(1)}}$ and $\\mathbf{b^{(1)}}$, i.e. $\\frac{\\partial l}{\\partial \\mathbf{W^{(1)}}}$, $\\frac{\\partial l}{\\partial \\mathbf{b^{(1)}}}$  (Show your work in details calculation steps. Answers without justification will not be credited.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>\n",
    "\n",
    "1.\n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\mathbf{W^{(2)}}} = \\frac{\\partial l}{\\partial \\mathbf{x^{(2)}}} \\frac{\\partial \\mathbf{x^{(2)}}}{\\partial z}\\frac{\\partial z}{\\partial W^{(2)}}$\n",
    "\n",
    "$ =-(y-x^{(2)})*f^{(2)'}(z)*x^{(1)} $\n",
    "\n",
    "$ =-(y-x^{(2)})*(f(z)*(1-f(z))*(x^{(1)}) $\n",
    "\n",
    "$ =-(0.01-0.7514)*(f(1.1059)*(1-f(1.1059))*([0.59326, 0.59688]) $\n",
    "\n",
    "$ = 0.7414*(0.7514*0.2486)*[0.59326, 0.59688] $\n",
    "\n",
    "$ = 0.1385*[0.59326, 0.59688] $\n",
    "\n",
    "$ = [0.0822, 0.0827] $\n",
    "\n",
    "\n",
    "\n",
    "$ \\frac{\\partial l}{\\partial b^{(2)}} = \\frac{\\partial l}{\\partial \\mathbf{x^{(2)}}} \\frac{\\partial \\mathbf{x^{(2)}}}{\\partial z}\\frac{\\partial z}{\\partial b^{(2)}}$\n",
    "\n",
    "$ =-(y-x^{(2)})*f^{(2)'}(z)*1 $\n",
    "\n",
    "$ =-(y-x^{(2)})*(f(z)*(1-f(z))*1 $\n",
    "\n",
    "$ =-(0.01-0.7514)*(f(1.1059)*(1-f(1.1059)) $\n",
    "\n",
    "$ = 0.7414*(0.7414*0.2486) $\n",
    "\n",
    "$ = 0.1385 $\n",
    "\n",
    "\n",
    "2. $\\frac{\\partial l}{\\partial \\mathbf{W^{(1)}}} = \\frac{\\partial l}{\\partial \\mathbf{x^{(2)}}} \\frac{\\partial \\mathbf{x^{(2)}}}{\\partial z}\\frac{\\partial z^{(2)}}{\\partial x^{(1)}}\\frac{\\partial \\mathbf{x^{(1)}}}{\\partial z^{(1)}}\\frac{\\partial \\mathbf{z^{(1)}}}{\\partial W^{(1)}}$\n",
    "\n",
    "$ =-(y-x^{(2)})*f^{(2)'}(z^{(2)})*W^{(2)}*f^{(1)'}(z^{(1)})*(x^{(0)}) $\n",
    "\n",
    "$ =-(0.01-0.7514)*(f(1.1059)*(1-f(1.1059))* [0.4,0.45]*(\\sum f([0.3775,0.3925])*(1-f([0.3775,0.3925]))*[0.05,0.1]$\n",
    "\n",
    "$ =0.1385*[0.4, 0.45]*([0.59326,0.59688]*[0.40674,0.40312])*[0.05, 0.1] $\n",
    "\n",
    "$ =0.1385*[0.0965, 0.1083]^T*[0.05, 0.1] $\n",
    "\n",
    "$ = \\begin{bmatrix}\n",
    "0.000668 & 0.001337 \\\\\n",
    "0.000750. & 0.001500\n",
    "\\end{bmatrix} $\n",
    "\n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\mathbf{b^{(1)}}} = \\frac{\\partial l}{\\partial \\mathbf{x^{(2)}}} \\frac{\\partial \\mathbf{x^{(2)}}}{\\partial z}\\frac{\\partial z^{(2)}}{\\partial x^{(1)}}\\frac{\\partial \\mathbf{x^{(1)}}}{\\partial z^{(1)}}\\frac{\\partial \\mathbf{z^{(1)}}}{\\partial b^{(1)}}$\n",
    "\n",
    "$ =-(y-x^{(2)})*f^{(2)'}(z^{(2)})*W^{(2)}*f^{(1)'}(z^{(1)})*1 $\n",
    "\n",
    "$ =-(0.01-0.7514)*(f(1.1059)*(1-f(1.1059))* [0.4,0.45]*( f([0.3775,0.3925])*(1-f([0.3775,0.3925]))$\n",
    "\n",
    "$ =0.1385*[0.4, 0.45]*([0.59326,0.59688]*[0.40674,0.40312]) $\n",
    "\n",
    "$ = 0.1385* [0.0965, 0.1083]$\n",
    "\n",
    "$ = [0.0134, 0.0150] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Coding a two-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define relative error function, which is used to check results later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example\n",
    "\n",
    "Before loading CIFAR-10, there will be a toy example to test your implementation of the forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3code.neural_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute forward pass scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-1.07260209  0.05083871 -0.87253915]\n",
      " [-2.02778743 -0.10832494 -1.52641362]\n",
      " [-0.74225908  0.15259725 -0.39578548]\n",
      " [-0.38172726  0.10835902 -0.17328274]\n",
      " [-0.64417314 -0.18886813 -0.41106892]]\n",
      "\n",
      "correct scores:\n",
      "[[-1.07260209  0.05083871 -0.87253915]\n",
      " [-2.02778743 -0.10832494 -1.52641362]\n",
      " [-0.74225908  0.15259725 -0.39578548]\n",
      " [-0.38172726  0.10835902 -0.17328274]\n",
      " [-0.64417314 -0.18886813 -0.41106892]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.381231233889892e-08\n"
     ]
    }
   ],
   "source": [
    "## Implement the forward pass of the neural network.\n",
    "\n",
    "# Note, there is a statement if y is None: return scores, which is why \n",
    "# the following call will calculate the scores.\n",
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "    [-1.07260209,  0.05083871, -0.87253915],\n",
    "    [-2.02778743, -0.10832494, -1.52641362],\n",
    "    [-0.74225908,  0.15259725, -0.39578548],\n",
    "    [-0.38172726,  0.10835902, -0.17328274],\n",
    "    [-0.64417314, -0.18886813, -0.41106892]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass loss\n",
    "\n",
    "The total loss includes data loss (MSE) and regularization loss, which is,\n",
    "\n",
    "$$L = L_{data}+L_{reg} = \\frac{1}{2N}\\sum_{i=1}^{N}\\left(\\boldsymbol{y}_{\\text{pred}}-\\boldsymbol{y}_{\\text{target}}\\right)^2 + \\frac{\\lambda}{2} \\left(||W_1||^2 + ||W_2||^2 \\right)$$\n",
    "\n",
    "More specifically in multi-class situation, if the output of neural nets from one sample is $y_{\\text{pred}}=(0.1,0.1,0.8)$ and $y_{\\text{target}}=(0,0,1)$ from the given label, then the MSE error will be $Error=(0.1-0)^2+(0.1-0)^2+(0.8-1)^2=0.06$\n",
    "\n",
    "Implement data loss and regularization loss. In the MSE function, you also need to return the gradients which need to be passed backward. This is similar to batch gradient in linear regression. Test your implementation of loss functions. The Difference should be less than 1e-12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "0.825637152507747\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss_MSE = 1.8973332763705641\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss_MSE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass (You do not need to implemented this part)\n",
    "\n",
    "We have already implemented the backwards pass of the neural network for you.  Run the block of code to check your gradients with the gradient check utilities provided. The results should be automatically correct (tiny relative error).\n",
    "\n",
    "If there is a gradient error larger than 1e-8, the training for neural networks later will be negatively affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 2.9632227682005116e-10\n",
      "b2 max relative error: 1.2482714253983918e-09\n",
      "W1 max relative error: 1.2832823337649917e-09\n",
      "b1 max relative error: 3.172680092703762e-09\n"
     ]
    }
   ],
   "source": [
    "from data.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('{} max relative error: {}'.format(param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Implement neural_net.train() to train the network via stochastic gradient descent, much like the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.014497864587765886\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify CIFAR-10\n",
    "\n",
    "Do classification on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from data.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = './data/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SGD\n",
    "\n",
    "If your implementation is correct, you should see a validation accuracy of around 15-18%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 2.302642126664509\n",
      "iteration 100 / 1000: loss 2.3025989121411037\n",
      "iteration 200 / 1000: loss 2.302592429071954\n",
      "iteration 300 / 1000: loss 2.302565850212495\n",
      "iteration 400 / 1000: loss 2.3025579136346885\n",
      "iteration 500 / 1000: loss 2.3024960598650543\n",
      "iteration 600 / 1000: loss 2.3024177519858746\n",
      "iteration 700 / 1000: loss 2.302348862497426\n",
      "iteration 800 / 1000: loss 2.3023512681521794\n",
      "iteration 900 / 1000: loss 2.3022970074152407\n",
      "Validation accuracy:  0.234\n",
      "Test accuracy (subopt_net):  0.238\n"
     ]
    }
   ],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-5, learning_rate_decay=0.95,\n",
    "            reg=0.1, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n",
    "\n",
    "# Save this net as the variable subopt_net for later comparison.\n",
    "subopt_net = net\n",
    "test_acc = (subopt_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy (subopt_net): ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09, 0.145, 0.23, 0.225, 0.245]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats['train_acc_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "The training accuracy isn't great. It seems even worse than simple KNN model, which is not as good as expected.\n",
    "\n",
    "(1) What are some of the reasons why this is the case? Based on previous observations, please provide at least two possible reasons with justification.\n",
    "\n",
    "(2) How should you fix the problems you identified in (1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>\n",
    "\n",
    "1. The number of iterations might be too small, and the learning rate might also be too small, such that the the training hasn't converged and reached its optimization till the end.\n",
    "\n",
    "2. We can increase the number of iterations and do more training. Also, we can increase the learning rate or decrease the learning rate decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the neural network\n",
    "\n",
    "Use the following part of the Jupyter notebook to optimize your hyperparameters on the validation set.  Store your nets as best_net. To get the full credit of the neural nets, you should get at least **45%** accuracy on validation set. \n",
    "\n",
    "**Reminder: Think about whether you should retrain a new model from scratch every time your try a new set of hyperparameters. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 2.302613099299309\n",
      "iteration 100 / 5000: loss 1.8491024705252783\n",
      "iteration 200 / 5000: loss 1.6352105255164477\n",
      "iteration 300 / 5000: loss 1.7522341464153315\n",
      "iteration 400 / 5000: loss 1.737992220086408\n",
      "iteration 500 / 5000: loss 1.5579737660528779\n",
      "iteration 600 / 5000: loss 1.7307495527041616\n",
      "iteration 700 / 5000: loss 1.56974815380458\n",
      "iteration 800 / 5000: loss 1.458100769175564\n",
      "iteration 900 / 5000: loss 1.6929654207259226\n",
      "iteration 1000 / 5000: loss 1.6318395005046897\n",
      "iteration 1100 / 5000: loss 1.5697640394951429\n",
      "iteration 1200 / 5000: loss 1.291384570411332\n",
      "iteration 1300 / 5000: loss 1.592513875783619\n",
      "iteration 1400 / 5000: loss 1.3317720764873384\n",
      "iteration 1500 / 5000: loss 1.5473938624817118\n",
      "iteration 1600 / 5000: loss 1.3358468148853155\n",
      "iteration 1700 / 5000: loss 1.3768537857181662\n",
      "iteration 1800 / 5000: loss 1.3728783574851438\n",
      "iteration 1900 / 5000: loss 1.597080856082713\n",
      "iteration 2000 / 5000: loss 1.1431784568960452\n",
      "iteration 2100 / 5000: loss 1.408229469379233\n",
      "iteration 2200 / 5000: loss 1.473010067946601\n",
      "iteration 2300 / 5000: loss 1.3729333888856168\n",
      "iteration 2400 / 5000: loss 1.3580451822758066\n",
      "iteration 2500 / 5000: loss 1.2265784818124281\n",
      "iteration 2600 / 5000: loss 1.4582302984341247\n",
      "iteration 2700 / 5000: loss 1.203473359774713\n",
      "iteration 2800 / 5000: loss 1.2570913704170577\n",
      "iteration 2900 / 5000: loss 1.2644986650180823\n",
      "iteration 3000 / 5000: loss 1.3048854767027758\n",
      "iteration 3100 / 5000: loss 1.2486204175091118\n",
      "iteration 3200 / 5000: loss 1.1850026029881509\n",
      "iteration 3300 / 5000: loss 1.1904702578084876\n",
      "iteration 3400 / 5000: loss 1.32147245937113\n",
      "iteration 3500 / 5000: loss 1.2676651756618063\n"
     ]
    }
   ],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "# ================================================================ #\n",
    "# START YOUR CODE HERE:\n",
    "# ================================================================ #\n",
    "#   Optimize over your hyperparameters to arrive at the best neural\n",
    "#   network.  You should be able to get over 45% validation accuracy.\n",
    "#   For this part of the notebook, we will give credit based on the\n",
    "#   accuracy you get.  Your score on this question will be multiplied by:\n",
    "#      min(floor((X - 23%)) / %22, 1) \n",
    "#   where if you get 50% or higher validation accuracy, you get full\n",
    "#   points.\n",
    "#\n",
    "#   Note, you need to use the same network structure (keep hidden_size = 50)!\n",
    "# ================================================================ #\n",
    "\n",
    "# todo: optimal parameter search (you may use grid search by for-loops )\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "best_valacc = 0\n",
    "best_net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = best_net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=5000, batch_size=100,\n",
    "            learning_rate=3e-3, learning_rate_decay=0.7,\n",
    "            reg=0.05, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "best_valacc = (best_net.predict(X_val) == y_val).mean()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save this net as the variable subopt_net for later comparison.\n",
    "# subopt_net = net\n",
    "\n",
    "# print(best_net.params)\n",
    "\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "# Output your results\n",
    "print(\"== Best parameter settings ==\")\n",
    "# print your best parameter setting here!\n",
    "print(\"Best accuracy on validation set: {}\".format(best_valacc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quesions**\n",
    "\n",
    "(1) What is your best parameter settings? (Output from the previous cell)\n",
    "\n",
    "(2) What parameters did you tune? How are they changing the performance of nerural network? You can discuss any observations from the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>\n",
    "1. The best parameter setting is \"num_iters=3000, batch_size=100, learning_rate=3e-3, learning_rate_decay=0.6,reg=0.05, verbose=True\". It achieves the accuracy of 0.53.\n",
    "\n",
    "2. The increase of iterations makes the model to be trained for more times and converges more colsers to its optimization;\n",
    "\n",
    "    The decrease of learning rate decay makes the learning rate decreases faster than before, which prevents the model from going furthur away from its optimization;\n",
    "\n",
    "    The decreases of batch size makes the model be trained faster and makes the memory be utilized less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the weights of your neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from data.vis_utils import visualize_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.T.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(subopt_net)\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "What differences do you see in the weights between the suboptimal net and the best net you arrived at? What do the weights in neural networks probably learn after training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>\n",
    "\n",
    "The sub-optimal net doesn't have obvious different colors and shapes among each two classifications. However, the best net has obvious shape and colors which means the weights of the params are clearly diffferentiated, which can do the classification in a more meaningful and clear way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy (best_net): ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "(1) What is your test accuracy by using the best NN you have got? How much does the performance increase compared with kNN? Why can neural networks perform better than kNN?\n",
    "\n",
    "(2) Do you have any other ideas or suggestions to further improve the performance of neural networks other than the parameters you have tried in the homework? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>\n",
    "\n",
    "1. The test accuracy for the best NN is 0.473. This accuracy is higher than the accuracy achieved by the KNN, which was 0.282. The neural networks can perform better because the existance of hidden layer and activation functions adds non-linearity to the model, which can classify features in higher dimensions and in a more complex way. In contrast, KNN only evaluates classification based on neighbors of each pixel, which cannot be very accurate.\n",
    "\n",
    "2. We can add more hidden layers and manipulates the number of neurons so that we can detect higher level/dimension features, and adds complexity to the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question: Change MSE Loss to Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bonus question. If you finish this (cross entropy loss) correctly, you will get **up to 10 points** (add up to your HW3 score). \n",
    "\n",
    "Note: From grading policy of this course, your maximum points from homework are still 25 out of 100, but you can use the bonus question to make up other deduction of other assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass output scores in networks from forward pass into softmax function. The softmax function is defined as,\n",
    "$$p_j = \\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{c=1}^{C} e^{z_c}}$$\n",
    "After softmax, the scores can be considered as probability of $j$-th class.\n",
    "\n",
    "The cross entropy loss is defined as,\n",
    "$$L = L_{\\text{CE}}+L_{reg} = \\frac{1}{N}\\sum_{i=1}^{N} \\log \\left(p_{i,j}\\right)+ \\frac{\\lambda}{2} \\left(||W_1||^2 + ||W_2||^2 \\right)$$\n",
    "\n",
    "To take derivative of this loss, you will get the gradient as,\n",
    "$$\\frac{\\partial L_{\\text{CE}}}{\\partial o_i} = p_i - y_i $$\n",
    "\n",
    "More details about multi-class cross entropy loss, please check [http://cs231n.github.io/linear-classify/](http://cs231n.github.io/linear-classify/) and [more explanation](https://deepnotes.io/softmax-crossentropy) about the derivative of cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the loss from MSE to cross entropy, you only need to change you ```MSE_loss(x,y)``` in ```TwoLayerNet.loss()``` function to ```softmax_loss(x,y)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you are free to use any code to show your results of the two-layer networks with newly-implemented cross entropy loss. You can use code from previous cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Start training your networks and show your results\n",
    "# ================================================================ #\n",
    "# START YOUR CODE HERE:\n",
    "# ================================================================ #\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "best_valacc2 = 0\n",
    "best_net2 = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats2 = best_net2.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=5000, batch_size=100,\n",
    "            learning_rate=3e-3, learning_rate_decay=0.6,\n",
    "            reg=0.05, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "best_valacc2 = (best_net2.predict(X_val) == y_val).mean()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats2['loss_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats2['train_acc_history'], label='train')\n",
    "plt.plot(stats2['val_acc_history'], label='val')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()\n",
    "print(\"Best accuracy on validation set: {}\".format(best_valacc2))\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## End of Homework 3, Part 2 :)\n",
    "\n",
    "After you've finished both parts the homework, please print out the both of the entire `ipynb` notebooks and `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. Do not include any dataset in your submission.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}