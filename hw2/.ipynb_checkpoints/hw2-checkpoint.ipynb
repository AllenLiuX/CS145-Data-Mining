{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 2\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW2 is due on **11:59 PM PT, Oct 30 (Friday, Week 4)**. Please submit through GradeScope. \n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Wenxuan Liu, UID: 805152602** </span>\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW2 conda environment by the given `cs145hw2.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw2.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw2.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks (such as some important hyperparameters) that you are allowed to edit (between STRART/END YOUR CODE HERE), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys \n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision trees\n",
    "This workbook will walk you through a decision tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Attribute selection measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification models, misclassification rate is usually used as the final performance measurement. However, for classification trees, when selecting which attribute to split, measurements people often use includes information gain, gain ratio, and Gini index. Let's investigate these different measurements through the following problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: below shows how to calculate the misclassification rate of a classification tree with $N$ total data points, $K$ classes of the value we want to predict, and $M$ leaf nodes. \n",
    "\n",
    "In a node $m$, $m = 1, ..., M$, let's denote the number of data points using $N_m$, and the number of data points in class k as $N_{mk}$, so the class prediction under majority vote is $j = argmax_k N_{mk}$. The misclassification rate of this node m is $R_m = 1 - \\frac{N_{mj}}{N_m}$. The total misclassification rate of the tree will be $R = \\frac{\\sum_{m=1}^M R_m * N_m}{N}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Questions**\n",
    "\n",
    "<span style=\"color:red\"> Note: this question is a pure \"question answer\" problem. You don't need to do any coding. </span>\n",
    "\n",
    "Suppose our dataset includes a total of 800 people with 400 males and 400 females, and our goal is to do gender classification. Consider two different possible attributes we can split on in a decision tree model. Split on the first attribute results in a node11 with 300 male and 100 female, and a node12 with 100 male and 300 female. Split on the second attribute results in in a node21 with 400 male and 200 female, and a node22 with 200 female only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which split do you prefer when the measurement is misclassifcation rate and why?\n",
    "2. What is the entropy in each of these four node? \n",
    "3. What is the information gain of each of the two splits?\n",
    "4. Which split do you prefer if the measurement is information gain. Do you see why it is an uncertainty or impurity measurement?\n",
    "5. What is the gain ratio (normalized information gain) of each of the two splits? Which split do you prefer under this measurement. Do you get the same conclusion as information gain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Note: you can use several code cells to help you compute the results and answer the questions. Again you don't need to do any coding. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 1\n",
    "<span style=\"color:blue\">\n",
    "1. Split on the first attribute has misclassification rate $$R = \\frac{\\sum_{m=1}^M R_m * N_m}{N}=\\frac{{\\frac{100}{400} * 400+\\frac{100}{400}*400}}{1600}=1/8$$.\n",
    "\n",
    "Split on the secound attribute has misclassification rate $$R = \\frac{\\sum_{m=1}^M R_m * N_m}{N}=\\frac{{\\frac{200}{600} * 600+\\frac{0}{200}*200}}{1600}=1/8$$\n",
    "\n",
    "Since they are the same, there is no preference on which split to use when misclassification rate is the measurement.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 2\n",
    "<span style=\"color:blue\">\n",
    "2. Since entropy is $$ Info(D)=-\\sum_{i=1}^mp_ilog_2(p_i)$$, we have the entropy for each node as:\n",
    "\n",
    "$$Info(node11)=-\\frac34log(\\frac34)-\\frac14log(\\frac14)=0.811$$\n",
    "\n",
    "$$Info(node12)=-\\frac34log(\\frac34)-\\frac14log(\\frac14)=0.811$$\n",
    "\n",
    "$$Info(node21)=-\\frac23log(\\frac23)-\\frac13log(\\frac13)=0.918$$\n",
    "\n",
    "$$Info(node22)=-1log(1)=0$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 3\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "First, we have that the entropy for the dataset is $$Info(root)=-\\frac12log(\\frac12)-\\frac12log(\\frac12)=1$$\n",
    "\n",
    "Then, the information gain for the split on the first attribute is $$ Gain(first)=1-(\\frac{400}{800}*0.811+\\frac{400}{800}*0.811)=0.187 $$\n",
    "\n",
    "The information gain for the split on the second attribute is $$ Gain(second)=1-(\\frac{600}{800}*0.918+\\frac{200}{800}*0)=0.311 $$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 4\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "I prefer split on the first attribute, because it has more information gain (0.811 > 0.311).\n",
    "\n",
    "Yes. For example, we can see that when there are 400 females and 400 males in the root node, we are totally uncertain about the classification, and correspondingly the entropy is 1; Also, there are 200 females in the node22 and we are certain that the classification for it is female and there is no impurity, and correspondingly it has entropy of 0. Thus, we can say that the entropy is a measure of uncertainty and impurity.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 5\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "The split info of first split is: $ SplitInfo(first)= -\\frac12log(\\frac12)-\\frac12log(\\frac12)=1 $\n",
    "\n",
    "And the gain ratio of the first split is: $ GainRatio(first) = \\frac{Gain(first)}{SplitInfo(first)}= 0.187$\n",
    "\n",
    "The split info of second split is: $ SplitInfo(second)= -\\frac34log(\\frac34)-\\frac14log(\\frac14)=0.811 $\n",
    "\n",
    "And the gain ratio of the first split is: $ GainRatio(first) = \\frac{Gain(first)}{SplitInfo(first)}= \\frac{0.311}{0.811}=0.383$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Coding decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to use the decision tree model to predict the the animal `type` class of the `zoo` dataset. The dataset has been preprocessed and splited into `decision-tree-train.csv` and `decision-tree-test.csv` for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (80, 17)\n",
      "Testing data shape: (21, 17)\n"
     ]
    }
   ],
   "source": [
    "from hw2code.decision_tree import DecisionTree\n",
    "mytree = DecisionTree()\n",
    "mytree.load_data('./data/decision-tree-train.csv','./data/decision-tree-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (80, 17) and testing data (21, 17)\n",
    "print('Training data shape: ', mytree.train_data.shape)\n",
    "print('Testing data shape:', mytree.test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Infomation gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `make_tree` and `compute_info_gain` function in `decision_tree.py`. \n",
    "\n",
    "Train you model using `info_gain` measure to classify `type` and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_feature is:  legs\n",
      "best_feature is:  fins\n",
      "best_feature is:  toothed\n",
      "best_feature is:  eggs\n",
      "best_feature is:  hair\n",
      "best_feature is:  hair\n",
      "best_feature is:  toothed\n",
      "best_feature is:  aquatic\n",
      "Test accuracy is:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "mytree = DecisionTree()\n",
    "mytree.load_data('./data/decision-tree-train.csv','./data/decision-tree-test.csv')\n",
    "test_acc= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "mytree.train('type', 'info_gain')\n",
    "test_acc = mytree.test('type')\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Gain ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `compute_gain_ratio` function in `decision_tree.py`. \n",
    "\n",
    "Train you model using `gain_ratio` measure to classify `type` and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_feature is:  feathers\n",
      "best_feature is:  backbone\n",
      "best_feature is:  airborne\n",
      "best_feature is:  predator\n",
      "best_feature is:  milk\n",
      "best_feature is:  fins\n",
      "best_feature is:  legs\n",
      "Test accuracy is:  0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "mytree = DecisionTree()\n",
    "mytree.load_data('./data/decision-tree-train.csv','./data/decision-tree-test.csv')\n",
    "test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "mytree.train('type', 'gain_ratio')\n",
    "test_acc = mytree.test('type')\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Which measure do you like the most and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "I prefer using info gain as the measurement. Because info gain is biased towards attributes with a large number of values, while th gain ratio use normalization to solve the problem.\n",
    "As we observe the dataset, the features in X basically have similar mean and standard error, and there are not a large number of values, so we do not need to spend more calculation on normalizing it.. Also, gain ratio tends to prefer unbalanced split, which indeed has such bias in the dataset, so we prefer info gain over gain ratio.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM \n",
    "This workbook will walk you through a SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Support vectors and decision boundary\n",
    "\n",
    "<span style=\"color:red\"> Note: for this question you can work entirely in the Jupyter Notebook, no need to edit any .py files. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider classifying the following 20 data points in the 2-d plane with class label y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.52</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.48</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1    x2  y\n",
       "0  0.52 -1.00  1\n",
       "1  0.91  0.32  1\n",
       "2 -1.48  1.23  1\n",
       "3  0.01  1.44  1\n",
       "4 -0.46 -0.37  1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('data/svm-2d-data.csv')\n",
    "ds.head()\n",
    "# This command above will print out the first five data points\n",
    "# in the dataset with column names as \"x1\", \"x2\" and \"y\"\n",
    "# You may use command \"ds\" to show the entire dataset, which contains 20 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose by solving the dual form of the quadratic programming of svm, we can derive the $\\alpha_i$’s for each data point as follows: Among $j=0,1,\\cdots, 19$ (note that the index starts from 0), $\\alpha_1$ = 0.5084, $\\alpha_5$ = 0.4625, $\\alpha_{17}$ = 0.9709, and $\\alpha_j$ = 0 for all other $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which vectors in the training points are support vectors?\n",
    "2. What is the normal vector of the hyperplane $w$? \n",
    "3. What is the bias $b$? \n",
    "4. With the parameters $w$ and $b$, we can now use our SVM to do predictions. What is predicted label of $x_{new} = (2,-0.5)$? Write out your $f(x_{new})$.\n",
    "5. A plot of the data points has been generated for you. Please change the `support_vec` variable such that only the support vectors are indicated by red circles. Please also fill in the code to draw the decision boundary. Does your prediction of part 4 seems right visually on the plot?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Note: you can use several code cells to help you compute the results and answer the questions. Again you don't need to edit any .py files. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "Please type your answer here!\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 1\n",
    "\n",
    "The support vectors are data points with non-zero values, which are data points 1, 5, and 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 2\n",
    "\n",
    "The solution for w is $$ w=\\sum a_i y_i x_i $$. Then, we can compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.338076 -0.388998]\n"
     ]
    }
   ],
   "source": [
    "w = 0.5084*ds.iloc[1, 2]*ds.iloc[1, :2].to_numpy() + 0.4625*ds.iloc[5, 2]*ds.iloc[5, :2].to_numpy() + 0.9709*ds.iloc[17, 2]*ds.iloc[17, :2].to_numpy()\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the normal vector $w=[ -1.338076, -0.388998 ] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 3\n",
    "\n",
    "The bias is calculated by $ b=y_k - w^Tx_k$. So, we can compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.342136106666666\n"
     ]
    }
   ],
   "source": [
    "b = ds.iloc[1, 2]-w.T @ ds.iloc[1, :2].to_numpy()\n",
    "b+= ds.iloc[5, 2]-w.T @ ds.iloc[5, :2].to_numpy()\n",
    "b+= ds.iloc[17, 2]-w.T @ ds.iloc[17, :2].to_numpy()\n",
    "b/=3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So we have that $ b=2.34213610666666 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 4\n",
    "\n",
    "Yes, we can do the prediction by the formula $f(x)=w^Tx+b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1395168933333335\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2, -0.5])\n",
    "f = w.T @ x + b\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we have that $f(x_{new})=-0.13951689 < 0$, wee can predict that the label y for $ x_{new} $ is $y=-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAHSCAYAAADbkg78AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABA2klEQVR4nO3deZhcZYG+//ut3judrauzQAKEVEBAQMKaLvw67cJI0FF0cBsXdHRww3FcJuIsOKKjDo7LuIzKpY7L+BM3VAZxUCGoIwEJ+45JCCEhkKSzdjrp9f39UQXpJN2kk66qU8v9ua66qDrn7ToPh1Tz5Jy3zgkxRiRJklQcqaQDSJIkVTPLliRJUhFZtiRJkorIsiVJklREli1JkqQismxJkiQVUTmXrVjMx7Jly4r6/rWWs5KymrN2s1ZKzkrKas7azWrO/R5jKueyVVR9fX1JRxiXSskJlZPVnIVXKVkrJSdUTlZzFl6lZDXn+NVs2ZIkSSoFy5YkSVIRWbYkSZKKyLIlSZJURJYtSZKkIrJsSZIkFZFlS5IkqYgsW5IkSUVk2ZIkSSoiy5YkSVIRWbYkSZKKyLIlSZJURJYtSZKkIrJsSZIkFVF90gEkSYXz4B//xPc+/hPWPLCWBafO5w3/9JccfdJRSceSapplS5KqxG2/vouPvOJy+nf1EyOsf2QDt/zidv79ho9w3JnHJB1PqlmeRpSkKvGl93yDvt5c0QKIw5G+3j6++oFvJxtMqnGWLUmqAgP9A6xb8cSo6x5evqrEaSSNNOGyFUJoDiH8MYRwVwjhvhDCR0cZ0xRC+EEIYUUI4ZYQwryJbleStEd9Qz3NrU2jrpvSMbnEaSSNVIgjW33AC2KMzwFOAc4NISzaZ8xbgS0xxgXA54B/K8B2JUl5IQRe9q4X09TauNfyptYmXvP3L0solSQoQNmKOT35lw35R9xn2MuBpyYN/Bh4YQghTHTbkqQ93vLx13HOG/+MxuYGWqe00NTSyPnvWcz57zkv6WhSTSvItxFDCHXAbcAC4Msxxlv2GTIHeAwgxjgYQtgGpIFNhdi+JAnq6ut471cu4q2ffD0bH9vErHkzaZ3cknQsqeaFGPc9CDWBNwthGvBT4D0xxntHLL8XODfGuDb/eiVwVoxx0z4/fxFwEcCSJUtOW7x4ccGy7aunp4e2traivX+hVEpOqJys5iy8SslaKTmhcrKas/AqJas599bV1TX2GbsYY0EfwKXAB/dZdh3QmX9eT+6IVjjAexXV0qVLi72JgqiUnDFWTlZzFl6lZK2UnDFWTlZzFl6lZDXnfsbsNIX4NuKM/BEtQggtwDnAg/sMuxq4MP/8AuCGGAt4SE2SJKlMFWLO1mHAt/PztlLAD2OM14QQLgOWxxivBr4BfDeEsALYDLy2ANuVJEkqexMuWzHGu4GFoyy/dMTz3cCrJrotSZKkSuMV5CVJkorIsiVJklREli1JkqQiKshFTSVJqgQ7tvTwm+/+ljUPrOPY0zN0vfZsWiY1Jx1LVc6yJUmqCWseXMd7z/5HBvoG6evto/m/f893PvpDvvzHT9E+e3rS8VTFPI0oSaoJn33bV9i5tZe+3j4Adu/czZYntvG1v/9uwslU7SxbkqSqF2PkgVv+xL7X0x4aHOLmq5cnlEq1wrIlSaoJITX6revqGupKnES1xrIlSap6IQTOXLyQuvq9i1VDUwN/fuGfJZRKtcKyJUmqCe+74h0cvmA2LW3NNLU00jypiWNOO5oLL/MOciouv40oSaoJ02dO5ev3fpY7b7iXdSueYP7JR3FC57GEMPrpRalQLFuSpJqRSqU49UUnc+qLTk46imqIpxElSZKKyLIlSZJURJYtSZKkIrJsSZIkFZFlS5IkqYgsW5IkSUVk2ZIkSSoiy5YkSVIRWbYkSZKKyLIlSZJURJYtSZKkIrJsSZIkFZFlS5IkqYjqkw4gSbVgzYPr+OU3rmfbxu2c9ZLTeO4rzqSuvi7pWJJKwLIlSUV2w/d/z2ff9lUGBwYZGhzm9z+5hZ998Vou/82lNDQ2JB1PUpF5GlGSimh3bx+fu+hr9O3qZ2hwOLds525W3PEI1//37xNOJ6kULFuSVET3L3uYVN3+v2p37+zjhu//XwKJJJWaZUuSiqippZEY46jrWic3lziNpCQ4Z0uSiuj4RcfQOrmFXTt277W8eVITL7nonIRSHbz7lz3Edz76I9Y8sJajTzqKN/3Lq3nW6ZmkY0kVwbIlSUWUSqX4+DUf5kPnfIzBgUGGhyNDA0Ocf/Fizjh3YdLxxmX5r+7iX15xOX27+gHYtLabu268l0/+8p8STiZVBsuWJBXZglOO5sp1X+O2X93Nji09PKfr2cw8oiPpWOP25fd+8+miBRAj9PX285X3f4tXf3pxgsmkymDZknTIYozcv347j/cMJx2l7DU0NrDopaclHeOgDQ8Ps/ahx0ddt+ruR0ucRqpMTpCXdMiGI7zuipv539UDSUdRkaRSKSZNax113dT05BKnkSqTZUvSIatLBRbNT/NA91DSUVREf/l3L6WptWmvZU2tTbx6ycsTSiRVFsuWpAnJZtJs3BV5bHNv0lFUJK//p7/kL9755zS1NNLS1kxTaxN/+Xcv4ZXvfUnS0aSK4JwtSROSXZCb6L1sZTdHtI9+ukmVLZVK8fZPv4k3/cur6X58Cx1z2mne50iXpLF5ZEvShBwzs40pjXDTyk1JR1GRtUxqZu4xh1m0pIM04bIVQjgihLA0hHB/COG+EMJ7RxnTFULYFkK4M/+4dKLblVQeQggc317HTSu7x7xSuiTVskKcRhwEPhBjvD2EMBm4LYTw6xjj/fuM+32M8aUF2J6kMnN8uo5bnuhj5cadLJjZlnQcSSorEz6yFWNcH2O8Pf98B/AAMGei7yupcpyQrgNgmacSJWk/oZCH/UMI84DfASfGGLePWN4F/ARYCzwOfDDGeN8oP38RcBHAkiVLTlu8uHhXJu7p6aGtrfz/Bl4pOaFyspqz8Hbs6OFfbktx9NQUFy8s35srV9I+rZSs5iy8Sslqzr11dXWFMVfGGAvyANqA24BXjrJuCtCWf34e8KdxvGdRLV26tNibKIhKyRlj5WQ1Z+EtXbo0fuCHd8bnfPS6ODQ0nHScMVXaPq0E5iy8Sslqzv2M2WkK8m3EEEIDuSNX34sxXjVKodseY+zJP78WaAghVM6NwSQdUDaTZmvvAA88sf3AgyWphhTi24gB+AbwQIzxs2OMmZ0fRwjhzPx2uye6bUnlozOTBnLX25Ik7VGII1tnA28EXjDi0g7nhRDeEUJ4R37MBcC9IYS7gC8Ar43R74hL1eSwqS3M75jETZYtSdrLhC/9EGP8P2DsSWG5MV8CvjTRbUkqb52ZND+7Yx0DQ8M01HnNZEkCryAvqYCymQ529g9xz7ptSUeRpLJh2ZJUMIvmtwPO25KkkSxbkgom3dbEcbMne59ESRrBsiWpoLKZDpav3sLugaGko0hSWbBsSSqobCZN3+Awd6zZmnQUSSoLli1JBXXm/HZSwfskStJTLFuSCmpKcwMnzZ3m9bYkKc+yJangspk0dz62lZ19g0lHkaTEWbYkFVw2k2ZwOHLr6s1JR5GkxFm2JBXc6Ue101AXvN6WJGHZklQELY11LDxyuvO2JAnLlqQiyWbS3Pv4Nrb1DiQdRZISZdmSVBTZTAcxws2PeHRLUm2zbEkqilOOmEZzQ8p5W5JqnmVLUlE01qc4Y16790mUVPMsW5KKJpvp4OEne9i4oy/pKJKUGMuWpKLJZtIALFvlqURJtcuyJalonn34FCY313ufREk1zbIlqWjq61KcdXTa621JqmmWLUlFlc2kebS7l7VbepOOIkmJsGxJKqrsgvy8LY9uSapRli1JRXXszMmkJzVatiTVLMuWpKJKpQKLMrl5WzHGpONIUslZtiQVXTaT5ontu3lk086ko0hSyVm2JBVdNtMB4LcSJdUky5akopuXbuWwqc3O25JUkyxbkoouhEBnJs2yVd0MDztvS1JtsWxJKolspoPNO/t56MkdSUeRpJKybEkqic78fRKdtyWp1li2JJXEnGktzEu3ep9ESTWnPukAUs3p64OlS2HDBqivh/nz4ayzIISkkxVdZ6aDa+56nMGhYerr/LuepNpg2ZJK5fHH4Utfgm98A449Fo46CoaG4I47cqXrXe+Ct7wFWlqSTlo02Uya7/9xDfc+vp1TjpiWdBxJKgnLllQKt90Gf/EXcMEF8LvfwbOetWddjHDjjXD55fDd78I110A6nVjUYlo0/6l5W5ssW5JqhsfxpWJ7+GF4yUvgy1+GL3xh76IFudOHz38+/OIXkM3mxu7alUzWIpsxuYlnzZrs9bYk1RTLllRs738/fOhD8IpXPL0oDvcQ+24k9t1CjIO5hakU/Pu/w8yZ8JWvJBS2+DozaW5dvZm+waGko0hSSVi2pGJ65BG4+WZ4+9ufXjTc+2Pihixx6/uJW99B3Pj/iAP35VaGAJdcAl/9akKBiy+bSbN7YJg712xNOooklYRlSyqmb34T3vQmaG0FIA48BNsvA3ZD7IG4E4a7iZvfQowDuZ/p7MyN31GdF/88a36aVPB6W5Jqh2VLKqaHH4Yzznj6Zez9IdA/ysBB6P9D7mkIcPrpuUtEVKGpLQ2cOGeq87Yk1YwJl60QwhEhhKUhhPtDCPeFEN47ypgQQvhCCGFFCOHuEMKpE92uVBEGBqChYc/ruBUY3n9cjDDcs+d1Y2NuWZXqzKS547Et9PYPJh1FkoquEEe2BoEPxBhPABYB7w4hnLDPmMXAMfnHRUD1zv6VRpoxAx577OmXoflFEFpHGTgAjWfteblmTe7aW1Uqm+lgYCiyfPWWpKNIUtFNuGzFGNfHGG/PP98BPADM2WfYy4HvxJybgWkhhMMmum2p7F1wAXzrW3uOUjWdA/UnAk9duDTknre9i1A3I7do/Xr4wx9gypTS5y2RM+ZNpz4VnLclqSYU9K/OIYR5wELgln1WzQEeG/F6bX7Z+kJuXyo7L3xh7ppZf/gDPPe5hFAP7f8Fu39B3PVLSE0itL6W0Hjmnp+54gp4zWugri653EXW2ljPwiOneZ9ESTUhxALNCwkhtAG/Bf41xnjVPuuuAT4VY/y//OvrgQ/FGJfvM+4icqcZWbJkyWmLFy8uSLbR9PT00NbWVrT3L5RKyQmVk7XkOTdvzt2q57jjDnxqsKcHVq6E446jZ2CgIvYnHNo+/emf+rl65QBfemErkxpKc1/ISvkzCpWT1ZyFVylZzbm3rq6usX+RxRgn/AAagOuA94+x/mvA60a8fgg47ADvW1RLly4t9iYKolJyxlg5WRPJedllMS5YEOMtt4y+fmAgxv/+7xg7OmL81a9ijJWzP2M8tKzLVm6KR33omvir+54ofKAxVPs+TYI5C69SsppzP2N2mgmfRgwhBOAbwAMxxs+OMexq4OIQwpXAWcC2GKOnEFU7/vmf4cgj4dWvzl0h/q//Go4+OvdtxTvvzJ06nDs3d8ueM8884NtVg4VHTqOpPsVNKzdxzgmzko4jSUVTiDlbZwNvBO4JIdyZX/YPwJEAMcavAtcC5wErgF7gLQXYrlRZLrwQ3vAG+OUv4cor4aqrcqcV58+Hn/8cFi5MOmFJNdXXcca8dq+3JanqTbhsxdw8rGeccBFjjMC7J7otqeLV1cFLX5p7iM5Mmk9f9xCbevroaGtKOo4kFYVXkJeUmGwmDcDNqzy6Jal6WbYkJeakOVNpa6r3eluSqpplS1Ji6utSnHW087YkVTfLlqREdWbSPLJpJ49v3ZV0FEkqCsuWpERlMx0AHt2SVLUsW5ISddzsyUxvbXDelqSqZdmSlKhUKtCZSbNs5aan7jAhSVXFsiUpcZ2ZDh7ftptHu3uTjiJJBWfZkpS4p6635alESdXIsiUpcfM7JjFrShM3rdyUdBRJKjjLlqTEhRDIZjpYtrLbeVuSqo5lS1JZ6Myk6d7Zz8NP9iQdRZIKyrIlqSw8NW/rDys8lSipuli2JJWFudNbObK9lWXelFpSlbFsSSob2Uyam1d1MzTsvC1J1cOyJalsdGbS7Ng9yH2Pb0s6iiQVjGVLUtno9HpbkqqQZUtS2Zg5uZljZrZZtiRVFcuWpLKSzaS59ZHN9A8OJx1FkgrCsiWprHRmOtg1MMRda7cmHUWSCsKyJamsLJrfTghw0wpPJUqqDpYtSWVlWmsjzz58ivdJlFQ1LFuSyk4208Eda7ayq38o6SiSNGGWLUllpzOTpn9omNse3ZJ0FEmaMMuWpLJzxrx26lPBU4mSqoJlS1LZaWuq5zlHTPN6W5KqgmVLUlnKZtLcvXYr23cPJB1FkibEsiWpLHVm0gxHuPWRzUlHkaQJsWxJKkunHjmdxvqUpxIlVTzLlqSy1NxQx+lHTbdsSap4li1JZSubSfPA+u1s3tmfdBRJOmT1SQeQpLF0ZjqAh7l5VTfnnXRY0nFK7/e/h5/9DLq7oakJjj8e3vQmaG9POpmkg+CRLUll6+S5U5nUWFd719u68kq4/35429tg+nR43vPglFPgttsgk4G3vAWeeCLplJLGySNbkspWQ12KM49ur615W5ddBt/5Dnzuc/DggxDCnnXvfCds3Aif/jR0dsL118P8+clllTQuHtmSVNaymQ5WbdzJE9t2Jx2l+L7+dfje9+Cmm2Dy5L2L1lNmzIDLL4cPfhDOPRe2by99TkkHxbIlqax1ZtIALFtV5acS+/vhn/8ZfvQjmDkTgKGhIZb9z3K+9Lff5Hv/+hM2PDZiH7z73blTi//1X8nklTRuli1JZe2Ew6YwtaWBm1ZU+anEn/0MjjsOTj4ZgBgjS154GZ94/X/w8y/9ku99/Mf89fHv5db/vWPPz/zt38J//ifEmExmSeNi2ZJU1lKpQOf8NDet7CZWc6n43vdyE+Lztm/awUPLV7K7J3f6dKBvkL7efj7x+v9gcGAwN+jssyGVyk2cl1S2LFuSyl52QZp1W3fx2OZdSUcpnvXrc980zNuxuYe+3r79hg0PDvPw8pW5FyHkfmb9+lKllHQIClK2QgjfDCFsCCHcO8b6rhDCthDCnfnHpYXYrqTakM3P26rqS0CkUjA8/PTLMNrkeHKnFxuaGvYsGBrK/aykslWoT+i3gHMPMOb3McZT8o/LCrRdSTUgM6ONGZObqvsSEPPmwT33PP1y6owpNE9q2m/YpGmTyJwyL/diaCh3Pa5580oSUdKhKUjZijH+DthciPeSpH2FEMhmqnze1lveAl/96tOT3dumT6LrNWfT2NJIU0sjLZObaZs+iY/9/EOknjqS9b//C7Nnw7OfnWBwSQdSymPPnSGEu0IIvwwh+JtB0kHJZtJs6uljxYaepKMUxznnwI4dsHTp04s+8PV38p/L/423f+ZCPvD1d/GDdVewYOHRuZXDw/DZz+YudCqprIVC/S0xhDAPuCbGeOIo66YAwzHGnhDCecB/xBiPGWXcRcBFAEuWLDlt8eLFBck2mp6eHtra2or2/oVSKTmhcrKas/BKkXVj7zB//7tdvOH4Rl50VMOBf2AUZb9Pt2+H1avh2GPpGRx85qxr18LOnXDssaNf/LREyn6f5lVKTqicrObcW1dX19gfxBhjQR7APODecY5dDXQcYFxRLV26tNibKIhKyRlj5WRdunRpHB4eTjrGAVXK/oyxdFnP/tT18aLv3HrIP18R+/S7341x5sy49Ic/jHH79v3X33ZbjK94RYxnnRXjpk2lz7ePitinsXJyxlg5Wc25nzE7TUnujRhCmA08GWOMIYQzyZ2+rOKZripHcehJ4vZ/gcETiU++k9jURZjyUULdzKSjaZyymTTX3fckQ8ORulRyR3OK6g1vgAUL4L774Kij4C/+AubMgd27c7fxWb8e3v52+Lu/g9bWpNNKGoeClK0QwveBLqAjhLAW+AjQABBj/CpwAfDOEMIgsAt4bYzVOstV5SjGfmL3q2F4A/BsYAj6bswtm/ErQmhMOqLGIZvp4IfL1/LA+u2cOGdq0nGKZ9GiXLm6+2645hro7ob2drj0Unjxi6GuLumEkg5CQcpWjPF1B1j/JeBLhdiWdEh2/wbidmBoxMIhiNug7wZoPtCVS1QOOkdcb6uqy9ZT5s6Fd7wj6RSSJsgr4ak2DK2C2Lv/8rgLBleVPo8OyawpzWRmTOIP1X6fRElVxbKl2lB/DISW/ZeHFqhfUPo8OmTZTAe3rt7MwNDwgQdLUhmwbKk2NL0AUh3sfea8HlLp3DpVjGwmTW//EHev3Zp0FEkaF8uWakIIDYT0D6H5PHJ/7Juh+SWE9A8JoSRfylWBLJqfn7flqURJFcKypZoRUu2kpv071J9AavbdpKZ9mpBqTzqWDtL0SY2ccNiU6r5PoqSqYtmSVHGymTS3rdnC7oGhAw+WpIRZtiRVnOyCNP2Dw9z+6Jako0jSAVm2JFWcM+a1U5cKnkqUVBEsW5IqzuTmBk6eO5WbVm5KOookHZBlS1JFymbS3LV2Gz19g0lHkaRnZNmSVJGymQ6GhiO3PrI56SiqMGseXMeX3/tNPvLKy/mfr/6K3b19SUdSlfMCQ5Iq0mlHTaexLsVNKzfx/ONmJh1HFeKmq2/lE3/1eQb7BxkaHOb2X9/NVZ+/hi/98VNMmtKadDxVKY9sSapIzQ11nHrUNCfJa9yGBof497d8mb7efoYGc7d72r2zjycf3cRVn/9FwulUzSxbkipWNtPB/eu3s2Vnf9JRVAFW3/cYg6Ncm22gb4Df/XhZAolUKyxbkipWNpMmRrjlEY9u6cBaJ7cwPMYNzFs9hagismxJqlgnz51Ga2OdpxI1LofNn8XcZx1OKhX2Wt48qYnzL16cUCrVAsuWpIrVWJ/ijHntli2N20d/uoRZ82bS0tZM6+QWGpoaWPzWF9L1mmzS0VTFavLbiHFwFQw9yvATJ0JogdbXEtreQwiNSUeTdJCymTSf/OWDbNi+m5lTmpOOozI366gZfOvhL3DfHx5iy5NbOX7RscyYm046lqpczZWtOLSB2P0qiH8N9EPsh53fJg6uIkz/ctLxJB2kbKYDgGWrunn5KXMSTqNKkEqlOOn/HZ90DNWQmjuNGHu/C7EPiCOW7oa+3xEH1yQVS9IhOuHwKUxpruemFZ5KlFSeaq5sMXAPMMrXxEMDDK4oeRxJE1OXCiyan2bZKsuWpPJUe2Wr/jhGPXsaB6F+XqnTSCqAbCbNms29PLa5N+kokrSfmitbYdKbIDTts7QJGk8j1M9PJJOkicku2DNvS5LKTe2VrbrDCe3/DaEVCEATtJxPmP6fSUeTdIiOmdlGR1sjy7wEhKQyVHPfRgQIDc+Guo2EWfcBdYQQDvgzkspXCIHOTAc3rdxEjNHPtKSyUnNHtkYKod5fylKVyGbSPLm9j1WbdiYdRZL2UtNlS1L1yGZyF6b0avKSyo1lS1JVOLK9lTnTWli2clPSUSRpL5YtSVUhN28rzbKV3QwPxwP/gCSViGVLUtXIZtJs6R3gwSd2JB1Fkp5m2ZJUNTqfnrflqURJ5cOyJalqHDa1hfkdk7zelqSyYtmSVFU6M2lueWQzg0PDSUeRJMCyJanKZDMd9PQNcs+6bUlHkSTAsiWpyiya3w54vS1J5cOyJamqpNuaOG72ZOdtSSobli1JVSeb6eDW1ZvpGxxKOookWbYkVZ9sJk3f4DC3P7o16SiSZNmSVH3OnN9OKsCyVZ5KlJS8gpStEMI3QwgbQgj3jrE+hBC+EEJYEUK4O4RwaiG2K0mjmdLcwElzp3mfRElloVBHtr4FnPsM6xcDx+QfFwFfKdB2JWlU2UyaO9Zspbd/MOkokmpcQcpWjPF3wOZnGPJy4Dsx52ZgWgjhsEJsW5JGk82kGRyO3Lp6S9JRJNW4EGMszBuFMA+4JsZ44ijrrgE+FWP8v/zr64EPxRiX7zPuInJHvliyZMlpixcvLki20fT09NDW1la09y+USskJ48k6AAwDTSVKNLpK2aeVkhPKM2vfUORdv+nlxfMaePWzGoHyzDmWSslqzsKrlKzm3FtXV1cYa1190bd+EGKMVwBXPPWymNu68cYb6erqKuYmCqJScsLYWePgWuLWi2FwJYQ6CJMIUy8nNJ1d+pBUzj6tlJxQvllPW7GMtQNDdHU9FyjfnKOplKzmLLxKyWrO8SvVtxHXAUeMeD03v0xVLsYh4uY3wOCDQB/EXhjeSNzyLuLg2qTjqcplM2nuXbeNbb0DSUeRVMNKVbauBt6U/1biImBbjHF9ibatJPXfAnEbudOHIw0Sd/0wiUSqIdlMB8MRbnnES0BISk5BTiOGEL4PdAEdIYS1wEeABoAY41eBa4HzgBVAL/CWQmxXFWB4I6OfER6AIQ9uqrhOOWIazQ0pblrZzZ8/e3bScSTVqIKUrRjj6w6wPgLvLsS2VGEaToE4yi1TQguhMZk5W6odjfUpzpjX7n0SJSXKK8irqEL9UdDyEqBlxNJGSM2GlvOSiqUaks108NCTO9i4oy/pKJJqVFl9G1HVKUz5BDScTuz9Xm6CfPN5hElvJYTmpKOpBmQzaQBuXtXN5ISzSKpNli0VXQgpaL2A0HpB0lFUg559+BQmN9dz08puXtyedBpJtcjTiJKqWn1dirOOTnufREmJsWxJqnrZTJrV3b1079r3EiSSVHyWLUlVL7sgN2/rgc2jfDNWkorMsiWp6h07czLpSY080O2RLUmlZ9mSVPVSqcCiTJoHNg+Ru+yfJJWOZUtSTchm0mzeHVnd3Zt0FEk1xrIlqSZkMx0A3OS3EiWVmGVLUk2Yl26lvTlwk7fukVRili1JNSGEwPHtddy8spvhYedtSSody5ZU5mIcIsbBpGNUhePTKbp39vPwhh1JR5FUQyxbUtkaZHjLO4hPnkh88iSGN7+FOLg26VAV7fj2OgBuWuGpREmlY9mSylCMgzC4Cvp+CwzlHv3LiJtfTYy7ko5XsdItKealW523JamkLFtSOeq7kadL1tOGYbgXdv0ymUxVojPTwS2ruhkc8gKnkkrDsiWVo6FHgdHKQC9xaFWp01SVbCbNjr5B7nt8e9JRJNUIy5ZUjuqPYdSPZ2gl1B9X8jjVZNH83H0SPZW4x2MPreOuG+9jx5aepKNIVak+6QCSRtH4XOBhoAEYyC+sh1Q7NP95crmqwIzJTTxr1mRuWrmJd3Zlko6TqG2btvPPL/s3Vt29mvqGevr7BnjNkvN500deRQgh6XhS1fDIllSGQkhB/XxoeSWENgit0PxSQvuPCKEx6XgVrzOT5tbVm+kfrO15Wx9/zef4020r6evtZ+e2XgZ2D/Djz1zN/111S9LRpKpi2ZLKVorU1I+RmnU7qVl3kpp2OaEunXSoqpDNpNk9MMydj21NOkpiutdv4b5lDzE4MLTX8t07+/jRZ/4noVRSdbJsSao5Z81Pkwq1fZ/Eni091NfXjbpu2ya/PCAVkmVLUs2Z2tLAiXOm1vQk+bnHHk5dw/5lq76hjrNecmoCiaTqZdmSVJM6M2nuWLOFXf1DBx5cherq63jvf/4NTa2NT0+Gb2hqYHJ6Mq+75BUJp5Oqi99GlFSTspkOvvbbVSx/dDP/75gZScdJRNdrzmb20TP58eeuYcOjmzj1nJN5xXsWM7VjStLRpKpi2ZJUk86YN536VOAPK7prtmwBHHfmMfzT99+XdAypqnkaUVJNam2sZ+GR01i2qnbnbUkqDcuWpJrVmengnrVb2b574MCDJekQWbYk1axsJs1whD+u2px0FElVzLIlqWYtPHIaTfWpmr4EhKTis2xJqllN9XWcMa+9pi9uKqn4LFuSalpnJs2DT+ygu6cv6SiSqpRlS1JNy2Zy95u82XlbkorEsiWppp00ZyptTfWeSpRUNJYtSTWtvi7FWUe3s8xJ8pKKxLIlqeZ1ZtKs2rST9dt2JR1FUhWybEmqedlMB4BHtyQVhWVLUs07bvZkprc2eL0tSUVh2ZJU81KpQGcmzbKV3cQYk44jqcoUpGyFEM4NITwUQlgRQrhklPVvDiFsDCHcmX+8rRDblaRC6cx0sG7rLtZs7k06iqQqUz/RNwgh1AFfBs4B1gK3hhCujjHev8/QH8QYL57o9iSpGJ663tZNK7s5Kj0p4TSSqkkhjmydCayIMa6KMfYDVwIvL8D7SlLJzO+YxKwpTc7bklRwYaLzE0IIFwDnxhjfln/9RuCskUexQghvBj4JbAQeBt4XY3xslPe6CLgIYMmSJactXrx4QtmeSU9PD21tbUV7/0KplJxQOVnNWXiVkvVAOb92927u2zTEfzy/lRBCCZPtr1r2abmolJxQOVnNubeurq6xf2nEGCf0AC4Avj7i9RuBL+0zJg005Z+/HbhhHO9dVEuXLi32JgqiUnLGWDlZzVl4lZL1QDl/cOuaeNSHrokPPbG9NIGeQbXs03JRKTljrJys5tzPmJ2mEKcR1wFHjHg9N79sZKHrjjE+dZfXrwOnFWC7klRQT8/bWuGteyQVTiHK1q3AMSGEo0MIjcBrgatHDgghHDbi5cuABwqwXUkqqLnTWzmyvdV5W5IKasLfRowxDoYQLgauA+qAb8YY7wshXAYsjzFeDfxtCOFlwCCwGXjzRLcrScWQzaS59p71DA1H6lLJztuSVB0mXLYAYozXAtfus+zSEc8/DHy4ENuSpGLqzKS58tbHuP/x7Zw0d2rScSRVAa8gL0kjdD59vS3nbUkqDMuWJI0wc3Izx8xsc96WpIKxbEnSPrKZNLeu3kz/4HDSUSRVAcuWpLENDMD69bB6NezYkXSakunMdNDbP8Tda7cmHUVSFbBsSdrfihXwwQ/C4YfDwoXwvOfB7Nnw4hfDz38Og4NJJyyqRfPbCQFPJUoqCMuWpD1ihI99DDo7IZWCm2+GJ56ANWuguxve8Ab45CfhzDPh8ceTTls001obefbhU5wkL6kgLFuS9rj0UrjqKrj7brj8cshk9qxrboY3vhGWLYNXvSp3tGvjxuSyFlk208Htj25l98BQ0lEkVTjLlqSc3/4WvvtduO46OGzPTR9iHCAO79wzLgT48Ifhggvgne9MIGhpdGbS9A8Nc9ujW5KOIqnCWbYk5Xzxi/ChD8HMmQDE4V6Gt32I+ORC4obTGd64mNi/fM/4f/onWLoUHnssocDFdca8dupTwVOJkibMsiUpN//qhhtyc7Ly4taLYdcvgH5gCIZWEje/lTj4SG5AWxu8/vVwxRWJRC62tqZ6nnPENCfJS5owy5YkuP12WLQIJk8GIA4+Cv3LyRWtkfqJO7+15+XixXDLLaVKWXLZTJq7125jx+6BpKNIqmCWLUnQ05M7UvWUoccgNIwycAgG/7Tn5eTJsHPnKOOqQ2cmzdBw5NbVm5OOIqmCWbYkwdSpsGXERPD6BRD7RhnYCI0L97zcvBmmTCl6vKSceuR0GutT3LTCU4mSDp1lS1LuFOLy5bBhAwChbja0vBRoHjEoQGgitL5pz6Kf/ARe8IKSRi2l5oY6Tj9quvO2JE2IZUsSTJ8Or3wlfPObTy8KU/4V2i6G1CwIk6DpBYT0Twh1s3IDNm2Cq6+Gv/7rhEKXRjaT5v7129m8c9/5a5I0PpYtSTnveQ98/vPwp9ycrBDqSLVdRGrm70nNuoPU9K8Q6uflxsYI73sfvPrVkE4nFrkUOjMdANyyyqNbkg6NZUtSzimnwMc/Di96Edxzz9jj+vvhb/4md//Ez32uZPGScvLcqUxqrPNUoqRDZtmStMfb3gaf+AT82Z/lTiv++tewY0euYK1aBR/5CMybl5sY/+tfQ2tr0omLrqEuxZlHt3txU0mHzLIlaW+vfz08+ii8+MW5K8ofdhhMmgTZbG6e1q9+lbt/4shLRVS5bKaDlRt38uT23UlHkVSB6pMOIKkMTZ4Mb3977iE6M7l5actWdnP+wjkJp5FUaTyyJUkHcMJhU5ja0uCpREmHxLIlSQeQSgU656edJC/pkFi2JGkcsgvSrN2yi8c29yYdRVKFsWxJ0jhk8/O2PJUo6WBZtiRpHDIz2pgxuclTiZIOmmVLksYhhEA2k5u3FWNMOo6kCmLZkqRxymbSbNzRx8qNPUlH0T76+wb41qVX8urD/4bzp1/IJ9/4BTat8yikyoNlS5LGKZu/T6KnEsvPR1/5aX70mf9hyxNb2bmtlxuv/APvOuMSdm73Cw1KnmVLksbpiPZW5k5v4aYVlq1y8si9a7jrt/fRv6v/6WXDQ8P0bt/Fr761NMFkUo5lS5IOQjaTZtmqboaHnbdVLlbeuZpUav//nfX19nH/socTSCTtzbJVIWKMxMG1xOHNSUeRalo208G2XQPcv3570lGUd3hmFqN9Z6GxuYGjTphb+kDSPixbFSD2LSNu/DPipvOIG57HcPfriUMbko4l1aSR90lUeTh+0bEcvmAW9Q11ey2vb6jnvL95UUKppD0sW2Wvn7jlHTD8BLAb6IeB24mbL/Tr51ICZk1pJjNjkhc3LSMhBD79m49w1ktOo76hjrr6OhYsPJrP3PhR2mdPTzqeRH3SAXQAw5uBwX0WDsHwehi4ExoXJhBKqm3ZTAdX3b6WgaFhGur8O2s5mJKezL9c9ff07+5naHCIlraWpCNJT/O3RLmL/cDAKCtC/miXpFLLZtLs7B/i7rXbko6ifTQ2N1q0VHYsW+UuTAJG+cURB6H+xJLHkQSL5j81b8tTiZIOzLJV7lLTITUNaBixsAVaXkKoPyKhUFJtmz6pkRMOm+LFTSWNi2Wr7KUIHT+F1tdD6nCoWwCTLyFM+UTSwaSals2kWf7oFnYPDCUdRVKZK0jZCiGcG0J4KISwIoRwySjrm0IIP8ivvyWEMK8Q260VIdVOaso/kJp5I6kZ15Ka9DpCsCdLScouSNM/OMzta7YkHUVSmZvw/7FDCHXAl4HFwAnA60IIJ+wz7K3AlhjjAuBzwL9NdLuSlKQz5rVTlwpeb0vSARXi8MiZwIoY46oYYz9wJfDyfca8HPh2/vmPgReGEEIBti1JiZjc3MDJc6c6b0vSARWibM0BHhvxem1+2ahjYoyDwDYgXYBtS1Jispk0dz22lZ6+fa+FJ0l7hIlehTyEcAFwbozxbfnXbwTOijFePGLMvfkxa/OvV+bHbNrnvS4CLgJYsmTJaYsXL55QtmfS09NDW1tb0d6/UColJ1ROVnMWXqVkLXTO+7uHuPzW3bz/tCZOnlHYa0TX6j4tlkrJCZWT1Zx76+rqGvuMXYxxQg+gE7huxOsPAx/eZ8x1QGf+eT2wiXzRe4ZHUS1durTYmyiISskZY+VkNWfhVUrWQufc1T8Yj/mHa+O//uL+gr5vjLW7T4ulUnLGWDlZzbmfMTtNIU4j3gocE0I4OoTQCLwWuHqfMVcDF+afXwDcEKM39pNU2Zob6jj1qGneJ1HSM5pw2Yq5OVgXkzt69QDwwxjjfSGEy0IIL8sP+waQDiGsAN4P7Hd5CEmqRNlMB/c9vp2tvf1JR5FUpgoyySDGeC1w7T7LLh3xfDfwqkJsS5LKSTaT5rO/hptXbebcE2cnHUdSGfLKmJI0ASfPnUZrY533SZQ0JsuWJE1AY32KM+a1e70tSWOybEnSBGUzaf60oYcNO3YnHUVSGbJsSdIEZTMdAN66R9KoLFuSNEEnHD6FKc31li1Jo7JsSdIE1aUCi+annbclaVSWLUkqgGwmzZrNvTy2uTfpKJLKjGVLkgoguyA/b2uVR7ck7c2yJUkFcMzMNjraGp23JWk/li1JKoAQAp2ZDm5auQlv/SppJMuWJBVINpPmye19rNq0M+koksqIZUuSCiSbSQP4rURJe7FsSVKBHNneypxpLd4nUdJeLFuSVCC5eVtplq3sZnjYeVuScixbklRA2UyaLb0DPPjEjqSjSCoTli1JKqDOp+dteSpRUo5lS5IK6LCpLczvmOT1tiQ9zbIlSQXWmUlzyyObGRwaTjqKpDJg2ZKkAstmOujpG+SedduSjiKpDFi2JKnAFs1vB7zelqQcy5YkFVi6rYnjZk92krwkwLIlSUWRzXSwfPUWdg8MJR1FUsIsW5JUBNlMmr7BYe5YszXpKJISZtmSpCI4c347qYC37pFk2ZKkYpjS3MBJc6c5SV6SZUuSiiWbSXPnY1vZ2TeYdBRJCbJsqeDi0Abi4EpidGKwals2k2ZwOHLr6s1JR5GUIMuWCmiQ4e6/Im58AbH7L4kbssTd1ycdSkrM6Ue101AXvHWPVOMsWyqIGCMMroaBO4F+iL0QtxC3vo848FDC6aRktDTWsfDI6c7bkmqcZUuFMfgA0A/sOzeln9j7nQQCSeUhm0lz7+Pb2NY7kHQUSQmxbKkwhjcAYbQVMLSu1GmkspHNdBAj3PyIR7ekWmXZUmHUnwjEUVY0Q2O21GmksnHKEdNobkg5b0uqYZYtFUSo64BUO9AyYmkDpKYRWl+bVCwpcY31Kc6Y1+59EqUaZtlS4aRmE6Z+LHeUq+5IaH0joeNnhNSUpJNJicpmOnj4yR427uhLOoqkBNQnHUDVJbS8jNDysqRjSGUlm0kDsGxVNy97zuEJp5FUah7ZkqQie/bhU5jcXO99EqUaZdmSpCKrr0tx1tFpr7cl1SjLliSVQDaT5tHuXtZu6U06iqQSs2xJUglkF+TnbXl0S6o5EypbIYT2EMKvQwh/yv9z+hjjhkIId+YfV09km5JUiY6dOZn0pEbLllSDJnpk6xLg+hjjMcD1+dej2RVjPCX/8KtqkmpOKhVYlMnN24pxtAsA5/zsjnWc/akbuGfdNs7+1A387A7vwCBVuomWrZcD384//zZw/gTfT5KqVjaT5ontu3lk085R1//sjnV8+Kp7WLd1FwDrtu7iw1fdY+GSDsHgwCA/+szVrL7vMd5w9Lu4Ysl32blt9M9esU20bM2KMa7PP38CmDXGuOYQwvIQws0hhPMnuE1JqkjZTAfAmN9K/PR1D7FrYGivZbsGhvj0dQ8VPZtUbS571Wf49kd+wMDuAZ58dCM/++K1XLzoH+jvK/1N4cMzHc4GCCH8Bpg9yqp/BL4dY5w2YuyWGON+87ZCCHNijOtCCPOBG4AXxhhXjjLuIuAigCVLlpy2ePHig/l3OSg9PT20tbUV7f0LpVJyQuVkNWfhVUrWpHPGGPnAb3eRmZbi3ac077f+nnXbnn4+qwWe3LVn3UlzppYi4kFLep+OV6XkhMrJWs45+3b189iD64jDkelzp7Jlbe6zFVKBWUfNYHJ74XN3dXWFsdYd8AryMcYXjbUuhPBkCOGwGOP6EMJhwIYx3mNd/p+rQgg3AguB/cpWjPEK4IqnXh4o20TceOONdHV1FXMTBVEpOaFyspqz8Colaznk7Np4Jzc+tJHnPe/PSKX2/t38j5+64elTiB84aZDP3JP7FT1nWgvveX1XqaOOSzns0/E4lJxDg0Ok6lKEMOb/Q4uimvdpqfziil/zs3/+DX29/bz604v54d//8ul15/3Ni3jf195e0jwTPY14NXBh/vmFwM/3HRBCmB5CaMo/7wDOBu6f4HYlqSJlMx1s3tnPQ0/u2G/d37/4WbQ01O21rKWhjr9/8bNKFU/A/cse4u0LP8jiptfxF21v4Mvv/WYip5506GYc0UGqbv+K09jcwGGZsWY8Fc9Ey9angHNCCH8CXpR/TQjh9BDC1/NjjgeWhxDuApYCn4oxWrYk1aTO/H0SR5u3df7COXzylScxZ1oLkDui9clXnsT5C+eUNGMtW/PgOj50zsdYddejxBjp29XPtV+/nk+98QtJR9NBOO2ck5k8vW2/o8d19XW8+MKukueZ0I2oY4zdwAtHWb4ceFv++U3ASRPZjiRViznTWpiXbmXZyk289blH77f+/IVzOH/hHG688cayPXVYzX7071fvdxSrf1c/t1xzGxvXdjNjbjqhZDoYdfV1fPa3l/Hx136OEAKNzQ10zE1zyXf/lumzppU8z4TKliTp4HVmOrjmrscZHBqmfpRTHUrOI/c8yvDQ8H7LG5oaeHzlE5atCjLrqBl8cdknuP431/NfD32BGXPTJZ9/9xQ/5ZJUYtlMmh19g9z7+Pako2gfx56eoa6+br/l/bsHOOJZhyeQSBNVV1/HzCM6EitaYNmSpJJbNP+peVubEk6ifb3qgy+jsaVhr2VNrY08/3Vn0z571DvSSQdk2ZKkEpsxuYlnzZrsfRLL0GFHz+Lzv/84p7zgRBqaGpg2cwqvveQVvP+KdyQdTRXMOVuSlIDOTJorb11D3+AQTaOctlJy5p98FJ/+zUeSjqEq4pEtSUpANpNm98Awd67ZmnQUSUVm2ZKkBJw1P00qjH2fREnVw7IlSQmY2tLAiXOmOm9LqgGWLUlKSGcmzR2PbaG3fzDpKJKKyLIlSQnJZjoYGIosX70l6SiSisiyJUkJOWPedOpTwXlbUpWzbElSQlob61l45DSWeXFTqapZtiQpQZ2ZDu5Zt41tuwYOPFhSRbJsSVKCspk0wxH++MjmpKNIKhLLliQlaOGR02iqT3mfRKmKWbYkKUFN9XWcMa/d621JVcyyJUkJ68ykefCJHWzq6Us6iqQisGxJUsKymTQAN6/y6JZUjSxbkpSwk+ZMpa2p3uttSVXKsiVJCauvS3HW0c7bkqqVZUuSykBnJs0jm3by+NZdSUeRVGCWLUkqA9lMB4BHt6QqZNmSpDJw3OzJTG9tcN6WVIUsW5JUBlKpQGcmzbKVm4gxJh1HUgFZtiSpTHRmOnh8224e7e5NOoqkArJsSVKZeOp6W55KlKqLZUuSysT8jknMmtLkfRKlKmPZkqQyEUIgm+lg2cpu521JVcSyJUllpDOTpntnP+t6LFtStbBsSVIZyWbSnDFvOruHLFtStbBsSVIZmTu9lR+9I8uCaXVJR5FUIJYtSZKkIrJsSZIkFZFlS5IkqYgsW5IkSUVk2ZIkSSoiy5YkSVIRWbYkSZKKyLIllbEYIzEOJx1DkjQBEypbIYRXhRDuCyEMhxBOf4Zx54YQHgohrAghXDKRbUq1IMZdMLSe+ORziE8ez3D3a4gDDyQdS5J0CCZ6ZOte4JXA78YaEEKoA74MLAZOAF4XQjhhgtuVqlrccjHELcBuIMLAHcTNf0UcWp90NEnSQZpQ2YoxPhBjfOgAw84EVsQYV8UY+4ErgZdPZLtSNYuDj0D/H4F9Th/GfuLO7yaSSZJ06EKME7/ZaQjhRuCDMcblo6y7ADg3xvi2/Os3AmfFGC8eZexFwEUAS5YsOW3x4sUTzjaWnp4e2traivb+hVIpOaFyspZ9zrgDhtbSs6uDtpYn914X2qBuXiKxnknZ79O8SskJlZPVnIVXKVnNubeurq4w1rr6A/1wCOE3wOxRVv1jjPHnEwm2rxjjFcAVT70s5Hvv68Ybb6Srq6uYmyiISskJlZO13HPGwUeJm/6B3937dp534hdGrGmESW8hNfnNSUUbU7nv06dUSk6onKzmLLxKyWrO8Ttg2YoxvmiC21gHHDHi9dz8MkmjCPVHEZvOZu+z/AFCI6H1DUnFkiQdolJc+uFW4JgQwtEhhEbgtcDVJdiuVLHCtC9Aqh3CZKABGhcR0j8k1M1KOpok6SAd8MjWMwkhvAL4IjAD+EUI4c4Y44tDCIcDX48xnhdjHAwhXAxcB9QB34wx3jfh5FIVC6ERUrNJzbot6SiSpAmaUNmKMf4U+Okoyx8Hzhvx+lrg2olsS5IkqRJNqGxpfzHmronEwB2QmgnN5xBCc9KxJElSQixbBRRjP3HL22HgdogDEJpg+8cg/f8R6hckHU+SJCXAeyMWUOz9HvTfBnEXMAhxJ8RtxC3vSTqaJElKiGWrkHp/TO72KiNFGFpLHFybRCJJkpQwy1ZBDY2xPLDfrVckSVJNsGwVUsv5wCiT4etmQN0R+y+XJElVz7JVQGHSm6HhWRBa80uaIUwiTPs8IYx5yyRJklTF/DZiAYXQDO1XQt/viAO3E+pmQ/NLCampSUeTJEkJsWwVWAh10Px8QvPzk44iSZLKgKcRJUmSisiyJUmSVESWLUmSpCKybEmSJBWRZUuSJKmILFuSJElFZNmSJEkqIsuWJElSEVm2JEmSisiyJUmSVESWLUmSpCKybEmSJBWRZUuSJKmILFuSJElFZNmSJEkqohBjTDpDIkIIF8UYr0g6x4FUSk6onKzmLLxKyVopOaFyspqz8ColqznHr5aPbF2UdIBxqpScUDlZzVl4lZK1UnJC5WQ1Z+FVSlZzjlMtly1JkqSis2xJkiQVUS2XrbI/z5xXKTmhcrKas/AqJWul5ITKyWrOwquUrOYcp5qdIC9JklQKtXxkS5IkqehqpmyFEF4VQrgvhDAcQjj9GcatDiHcE0K4M4SwvJQZ89sfb85zQwgPhRBWhBAuKWXGERnaQwi/DiH8Kf/P6WOMG8rvzztDCFeXMN8z7qMQQlMI4Qf59beEEOaVKts+OQ6U880hhI0j9uHbEsr5zRDChhDCvWOsDyGEL+T/Pe4OIZxa6oz5HAfK2RVC2DZif15a6oz5HEeEEJaGEO7Pf+bfO8qYctmn48ma+H4NITSHEP4YQrgrn/Ojo4xJ/HM/zpxl8bnPZ6kLIdwRQrhmlHWJ78998jxT1uT2aYyxJh7A8cCzgBuB059h3Gqgo5xzAnXASmA+0AjcBZyQQNbLgUvyzy8B/m2McT0JZDvgPgLeBXw1//y1wA/KNOebgS+VOtsoWZ8HnArcO8b684BfAgFYBNxSpjm7gGvKYH8eBpyafz4ZeHiU//blsk/HkzXx/ZrfT2355w3ALcCifcaUw+d+PDnL4nOfz/J+4P8b7b9vOezPg8ia2D6tmSNbMcYHYowPJZ3jQMaZ80xgRYxxVYyxH7gSeHnx0+3n5cC388+/DZyfQIaxjGcfjcz/Y+CFIYRQwoxQPv8tDyjG+Dtg8zMMeTnwnZhzMzAthHBYadLtMY6cZSHGuD7GeHv++Q7gAWDOPsPKZZ+OJ2vi8vupJ/+yIf/Yd2Jy4p/7ceYsCyGEucBLgK+PMSTx/fmUcWRNTM2UrYMQgV+FEG4LISR+IbQxzAEeG/F6Lcn84psVY1yff/4EMGuMcc0hhOUhhJtDCOeXJtq49tHTY2KMg8A2IF2SdKNkyBvrv+Vf5k8j/TiEcERpoh20cvlzOR6d+VM4vwwhPDvpMPlTLwvJHeEYqez26TNkhTLYr/nTSHcCG4BfxxjH3KcJfu7HkxPK43P/eWAJMDzG+rLYn3mf55mzQkL7tKrKVgjhNyGEe0d5HMyRgufGGE8FFgPvDiE8r0xzlsR4s8bcMdqx/mZ2VIzxdOCvgM+HEDLFzl1l/geYF2M8Gfg1e/4WqUNzO7k/k88Bvgj8LMkwIYQ24CfA38UYtyeZ5UAOkLUs9muMcSjGeAowFzgzhHBiEjkOZBw5E//chxBeCmyIMd5W6m0frHFmTWyf1pdqQ6UQY3xRAd5jXf6fG0IIPyV3mud3E33ffbYx0ZzrgJGNfG5+WcE9U9YQwpMhhMNijOvzpzY2jPEeT+3TVSGEG8n9rXhlMfKOMJ599NSYtSGEemAq0F3kXPs6YM4Y48hMXyc3V64clezP5USMLAkxxmtDCP8ZQuiIMW4qdZYQQgO58vK9GONVowwpm316oKzltF/zGbaGEJYC5wIjvyxRDp/7p42Vs0w+92cDLwshnAc0A1NCCP8dY3zDiDHlsj8PmDXJfVpVR7YmKoQwKYQw+annwJ+z94e0XNwKHBNCODqE0EhuUmLJvuU3wtXAhfnnFwI/33dACGF6CKEp/7yD3Afi/hJkG88+Gpn/AuCG/BG6Ujpgzn3m6LyM3HyZcnQ18KaQswjYNuI0c9kIIcx+ak5JCOFMcr8HS/4/h3yGbwAPxBg/O8awstin48laDvs1hDAjhDAt/7wFOAd4cJ9hiX/ux5OzHD73McYPxxjnxhjnkfvddMM+RQvKYH/C+LImuk+LOfu+nB7AK8jNd+gDngSuyy8/HLg2/3w+uW+D3QXcB/xjOebMvz6P3DeCViaRM58hDVwP/An4DdCeX3468PX88yxwT36f3gO8tYT59ttHwGXAy/LPm4EfASuAPwLzE9qPB8r5yfyfx7uApcBxCeX8PrAeGMj/GX0r8A7gHfn1Afhy/t/jHp7hW78J57x4xP68GcgmlPO55E693w3cmX+cV6b7dDxZE9+vwMnAHfmc9wKX5peX1ed+nDnL4nM/InMX+W/4ldv+PIisie1TryAvSZJURJ5GlCRJKiLLliRJUhFZtiRJkorIsiVJklREli1JkqQismxJkiQVkWVLkiSpiCxbkiRJRfT/A7JABXvPMG/uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# answer 5\n",
    "x1_range = np.arange(-2, 5, 0.5)\n",
    "x2_range = np.arange(-2, 4., 0.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(x1_range)\n",
    "ax.set_yticks(x2_range)\n",
    "ax.grid()\n",
    "ax.scatter(ds['x1'], ds['x2'], c=ds['y'])\n",
    "\n",
    "support_vec = ds\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "support_vec = ds.iloc[[1,5,17],]\n",
    "xs = np.linspace(1, 2, 10)\n",
    "ys = -w[0]/w[1] * xs - b/w[1]\n",
    "ax.plot(xs,ys)\n",
    "ax.scatter(x[0], x[1])\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "ax.scatter(support_vec['x1'], support_vec['x2'], marker='o', facecolor='none', s=200, color='red')\n",
    "sns.despine(ax=ax, left=True, bottom=True, offset=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Yes, my prediction of part 4 seems right visually on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Coding SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to use SVM for classifying the `y` value of 4-dimensional data points. The dataset has been preprocessed and splited into `svm-train.csv` and `svm-test.csv` for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question we are going to use the `cvxopt` package to help us solve the optimization problem of SVM. You will see it in the .py files, but you don't need to any coding with it. For this question, you only need to implement the right kernel function, and your kernel matrix `K` in `svm.py` line 135 will be pluged in the cvxopt optimization problem solver.\n",
    "\n",
    "\n",
    "For more information about `cvxopt` please refer to http://cvxopt.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1098, 4) (1098,)\n",
      "Testing data shape: (274, 4) (274,)\n"
     ]
    }
   ],
   "source": [
    "from hw2code.svm import SVM\n",
    "svm = SVM()\n",
    "svm.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1098, 4) and (1098,) and testing data (274, 4) and (274,)\n",
    "print('Training data shape: ', svm.train_x.shape, svm.train_y.shape)\n",
    "print('Testing data shape:', svm.test_x.shape, svm.test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `SVM.predict` and `linear_kernel` function in `svm.py`. \n",
    "Train a hard margin SVM and a soft margin SVM with linear kernel. Print the test accuracy for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098 support vectors out of 1098 points\n",
      "38 support vectors out of 1098 points\n",
      "Hard margin test accuracy is:  0.5547445255474452\n",
      "Soft margin test accuracy is:  0.9890510948905109\n"
     ]
    }
   ],
   "source": [
    "svm_hard = SVM()\n",
    "svm_hard.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "hard_test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm_hard.train()\n",
    "hard_test_acc = svm_hard.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "\n",
    "svm_soft = SVM()\n",
    "svm_soft.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "soft_test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm_soft.train(C=1)\n",
    "soft_test_acc = svm_soft.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Hard margin test accuracy is: ', hard_test_acc)\n",
    "print('Soft margin test accuracy is: ', soft_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Are these two results similar? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please write down your answers and/or observations here </span>\n",
    "\n",
    "The result are not similar.\n",
    "\n",
    "For soft margin SVM, it allows some misclassified points to be moved to the right position at a cost, as well as allowing some points to be inside the margin, which can achieve the largest margin while a few points are misclassified. So, we have only 38 support vectors compared to 1098 support vectors for hard margin SVM, which means the soft margin SVM has larger margin and much better accuracy.\n",
    "\n",
    "For hard margin SVM, all the points must be outside the margin of the decision boundary. Since we have 1098 support vector, we can see that the data points are not separated very neatly by the hyperplane, which makes all all the points to be support vectors and the margin is almost 0. So it achieved very low test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `polynomial_kernel` function in `svm.py`. \n",
    "Train a soft margin SVM with degree 3 polynomial kernel and parameter `C = 100` for the regularization term. Print the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 support vectors out of 1098 points\n",
      "Test accuracy is:  0.927007299270073\n"
     ]
    }
   ],
   "source": [
    "svm = SVM()\n",
    "svm.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm.train(kernel_name='polynomial_kernel', C=100)\n",
    "test_acc = svm.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Is the result better than linear kernel? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please write down your answers and/or observations here </span>\n",
    "\n",
    "No. As we can see that the test accuracy for the linear kernel is around 0.98 while the the test accuracy for the polynomial_kernel is around 0.93.\n",
    "\n",
    "It is probably because the polynomial kernel transforms the data points into higher dimension and gives more complexity to the distribution of the data points, which makes the SVM harder to clearly separate the points by a hyperplane with large margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Gaussian kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the `gaussian_kernel` function using the `gaussian_kernel_point` in `svm.py`. \n",
    "Train a soft margin SVM with Gaussian kernel and parameter `C = 100` for the regularization term. Print the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 support vectors out of 1098 points\n",
      "Test accuracy is:  1.0\n"
     ]
    }
   ],
   "source": [
    "svm = SVM()\n",
    "svm.load_data('./data/svm-train.csv', './data/svm-test.csv')\n",
    "test_acc = 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "svm.train(kernel_name='gaussian_kernel', C=100)\n",
    "test_acc = svm.test()\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Test accuracy is: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Is the result better than linear kernel and polynomial kernel? Why or why not?\n",
    " 2. Which one of these four models do you like the most and why?\n",
    " 3. (Bonus question, optional) Can you come up with a vectorized implementation of `gaussian_kernel` without calling `gaussian_kernel_point`? Fill that in svm.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please write down your answers and/or observations here </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 1\n",
    "\n",
    "The result is better than linear kernel and polynomial kernel, as we can see that the test accuracy achieved 1.\n",
    "\n",
    "It is because the gaussian kernel can map the points to infinite dimension, which makes the soft margin SVM be able to separate the points perfectly with a hyperplane in infinite dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 2\n",
    "\n",
    "I like soft margin SVM with Gaussian kernel the most, because it can extend features to infinite dimensions and find a perfectly separable hyperplane that and fit all the points with appropriate margin, so it can basically fit all kinds of distributions. Comparatively, the linear kernel is mainly suitable for linearly separable points, and polynomial kernel can only extend features to limited dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 2 :)\n",
    "After you've finished the homework, please print out the entire `ipynb` notebook and two `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. Also this time remember assign the pages to the questions on GradeScope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
