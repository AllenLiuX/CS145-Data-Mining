{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 6, Naive Bayes and Topic Modeling\n",
    "\n",
    "<span style=\"color:red\"> **Due date:** </span>\n",
    "HW6 is due on **11:59 PM PT, Dec. 14 (Monday, Final Week)**. Please submit through GradeScope. \n",
    "\n",
    "----\n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: XXX, UID: XXX** </span>\n",
    "\n",
    "----\n",
    "\n",
    "## Important Notes about HW6\n",
    "\n",
    "* HW6, as the last homework, is optional if you choose to use the first 5 homework assignments for homework grading. We will select your highest 5 homework grades to calculate your final homework grade. \n",
    "* Since HW6 is optional, for the implementaion of Naive Bayes and pLSA, you can choose to implement the provided `.py` and `.py` file by filling in the blocks. <span style=\"color:red\"> Alternatively, you are given the option to implement completely from scratch based on your understanding. Note that some packages with ready-to-use implementation of Naive Bayes and pLSA are not allowed. </span>\n",
    "\n",
    "----\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW6 conda environment by the given `cs145hw6.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw6.yml\n",
    "conda activate hw6\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw6.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks (such as hyperparameters) that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import zeros, int8, log\n",
    "from pylab import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8,8\n",
    "import seaborn as sns; sns.set()\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `seaborn` in HW6 is only used for ploting classification confusion matrix (in a \"heatmap\" style). If you encounter installation problem and cannot solve it, you may use alternative plot methods to show your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Naive Bayes for Text (50 points)\n",
    "\n",
    "Naive Bayers is one generative model for text classification. In the problem, you are given a document in `dataset` folder. The original data comes from [\"20 newsgroups\"](http://qwone.com/~jason/20Newsgroups/). You can use the provided data files to save efforts on preprocessing.\n",
    "\n",
    "Note: The code and dataset are under the subfolder named `nb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data processing and preparation\n",
    "# read train/test labels from files\n",
    "train_label = pd.read_csv('./nb/dataset/train.label',names=['t'])\n",
    "train_label = train_label['t'].tolist()\n",
    "test_label = pd.read_csv('./nb/dataset/test.label', names=['t'])\n",
    "test_label= test_label['t'].tolist()\n",
    "\n",
    "# read train/test documents from files\n",
    "train_data = open('./nb/dataset/train.data')\n",
    "df_train = pd.read_csv(train_data, delimiter=' ', names=['docIdx', 'wordIdx', 'count'])\n",
    "test_data = open('./nb/dataset/test.data')\n",
    "df_test = pd.read_csv(test_data, delimiter=' ', names=['docIdx', 'wordIdx', 'count'])\n",
    "\n",
    "# read vocab\n",
    "vocab = open('./nb/dataset/vocabulary.txt') \n",
    "vocab_df = pd.read_csv(vocab, names = ['word']) \n",
    "vocab_df = vocab_df.reset_index() \n",
    "vocab_df['index'] = vocab_df['index'].apply(lambda x: x+1) \n",
    "\n",
    "# add label column to original df_train\n",
    "docIdx = df_train['docIdx'].values\n",
    "i = 0\n",
    "new_label = []\n",
    "for index in range(len(docIdx)-1):\n",
    "    new_label.append(train_label[i])\n",
    "    if docIdx[index] != docIdx[index+1]:\n",
    "        i += 1\n",
    "new_label.append(train_label[i])\n",
    "df_train['classIdx'] = new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the data prepared properly, the following line of code would return the head of the `df_train` dataframe, which is,\n",
    "\n",
    "\n",
    "|  |  docIdx  |   wordIdx |  count  | classIdx |\n",
    "| :---: | :---:        |    :----:   |      :---: | :---: |\n",
    "| 0 | 1 | 1 | 4 | 1 |\n",
    "| 1\t| 1 | 2 | 2 | 1 |\n",
    "| 2 | 1 | 3 | 10 | 1 |\n",
    "| 3 | 1 | 4 | 4 | 1 |\n",
    "| 4 | 1 | 5 | 2 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of 'df_train'\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the implementation of Naive Bayes model for text classification `nbm.py`.  After that,  run `nbm_sklearn.py`,  which uses `sklearn` to implement naive bayes model for text classification. (Note that the dataset is slightly different loaded in `nbm_sklearn.py` and also you don't need to change anything in `nbm_sklearn.py` and directly run it.) \n",
    "\n",
    "If the implementation is correct, you can expect the results are generally close on both train set accuracy and test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb.nbm import NB_model\n",
    "\n",
    "# model training\n",
    "nbm = NB_model()\n",
    "nbm.fit(df_train, train_label, vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on train set to validate the model\n",
    "predict_train_labels = nbm.predict(df_train)\n",
    "train_acc = (np.array(train_label) == np.array(predict_train_labels)).mean()\n",
    "print(\"Accuracy on training data by my implementation: {}\".format(train_acc))\n",
    "\n",
    "# make predictions on test data\n",
    "predict_test_labels = nbm.predict(df_test)\n",
    "test_acc = (np.array(test_label) == np.array(predict_test_labels)).mean()\n",
    "print(\"Accuracy on training data by my implementation: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classification matrix\n",
    "mat = confusion_matrix(test_label, predict_test_labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.title('Classification Performace by sklearn')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./nb/output/nbm_mine.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Reminder:** </span> Do not forget to run nbm_sklearn.py to compare the results to get the accuracy and confusion matrix by sklearn implementation. You can run `python nbm_sklearn.py` under the folder path of `./hw6/nb/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question & Analysis**\n",
    "\n",
    "0. Please indicate whether you implemented based the given code or from scratch. \n",
    "\n",
    "1. Report your classification accuracy on train and test documents. Also report your classification confusion matrix. Show one example document that Naive Bayes classifies incorrectly (i.e. fill in the following result table). Attach the output figure `./output/nbm_mine.png` in the jupyter book and briefly explain your observation on the accuracy and confusion matrix.\n",
    "\n",
    "|    |   Train set accuracy |  Test set accuracy  |\n",
    "| :---        |    :----:   |                ---: |\n",
    "| sklearn implementaion|          |               |\n",
    "| your implementaion   |          |               |\n",
    "\n",
    "2. Show one example document that Naive Bayes classifies incorrectly by filling the following table. Provide your thought on the reason why this document is misclassified. (Note that the topic mapping is available at `train.map` same as `test.map`)\n",
    "\n",
    "|  Words (count) in the example document  | Predicted label |  Truth label |\n",
    "| :---        |    :----:   |                ---: |\n",
    "| For example, student (4), education (2), ... |     Class A     |   Class B    |\n",
    "\n",
    "3. Is Naive Bayes a generative model or discriminative model and why? What is the difference between Naive Bayes classifier and Logistic Regression? What are the pros and cons of Naive Bayes for text classification task?\n",
    "\n",
    "4. Can you apply Naive Bayes model to identify spam emails from normal ones? Briefly explain your method (you don't need to implementation for this question)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers**\n",
    "\n",
    "<span style=\"color:blue\"> Type your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 2: Topic Modeling: Probabilistic Latent Semantic Analysis (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement Probabilistic Latent Semantic Analysis (pLSA) by EM algorithm. Note: The code and dataset are under the subfolder named `plsa`. You can find two dataset files named `dataset1.txt` and `dataset2.txt` together with a [stopword](https://en.wikipedia.org/wiki/Stop_word) list as `stopwords.dic`. \n",
    "\n",
    "First complete the implementation of pLSA in `plsa.py`. You need to finish the E step, M step and likelihood function. Note that the optimizing process on dataset 2 might take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file, outpot files and parameters\n",
    "datasetFilePath = './plsa/dataset/dataset1.txt' # or set as './plsa/dataset/dataset2.txt'\n",
    "stopwordsFilePath = './plsa/dataset/stopwords.dic'\n",
    "docTopicDist = './plsa/output/docTopicDistribution.txt'\n",
    "topicWordDist = './plsa/output/topicWordDistribution.txt'\n",
    "dictionary = './plsa/output/dictionary.dic'\n",
    "topicWords = './plsa/output/topics.txt'\n",
    "\n",
    "K = 4   # number of topic\n",
    "maxIteration = 20 # maxIteration and threshold control the train process\n",
    "threshold = 3\n",
    "topicWordsNum = 20 # parameter for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plsa.plsa import PLSA\n",
    "from plsa.utils import preprocessing\n",
    "\n",
    "N, M, word2id, id2word, X = preprocessing(datasetFilePath, stopwordsFilePath) # data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsa_model = PLSA()\n",
    "plsa_model.initialize(N, K, M, word2id, id2word, X)\n",
    "\n",
    "oldLoglikelihood = 1\n",
    "newLoglikelihood = 1\n",
    "\n",
    "for i in range(0, maxIteration):\n",
    "    plsa_model.EStep() #implement E step\n",
    "    plsa_model.MStep() #implement M step\n",
    "    newLoglikelihood = plsa_model.LogLikelihood()\n",
    "    print(\"[\",time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\"]\", i+1, \n",
    "          \"iteration\", str(newLoglikelihood))\n",
    "    # you should see increasing loglikelihood\n",
    "    if(newLoglikelihood - oldLoglikelihood < threshold):\n",
    "        break\n",
    "    oldLoglikelihood = newLoglikelihood\n",
    "    \n",
    "plsa_model.output(docTopicDist, topicWordDist, dictionary, topicWords, topicWordsNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsa_model.output(docTopicDist, topicWordDist, dictionary, topicWords, topicWordsNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question & Analysis**\n",
    "\n",
    "0. Please indicate whether you implemented based the given code or from scratch. \n",
    "1. Choose different $K$ (number of topics) in `plsa.py`. What is your option for a reasonable $K$ in `dataset1.txt` and `dataset2.txt`? Give your results of 10 words under each topic by filling in the following table (suppose you set $K=4$). \n",
    "\n",
    "For dataset 1:\n",
    "\n",
    "|  Topic 1  |   Topic 2 |  Topic 3  | Topic 4 |\n",
    "| :---        |    :----:   |      :---: | :---: |\n",
    "|      *your words*     |    *your words*      |       *your words*      |    *your words*   |\n",
    "\n",
    "For dataset 2:\n",
    "\n",
    "|  Topic 1  |   Topic 2 |  Topic 3  | Topic 4 |\n",
    "| :---        |    :----:   |      :---: | :---: |\n",
    "|      *your words*     |    *your words*      |       *your words*      |    *your words*   |\n",
    "\n",
    "2. Are there any similarities between pLSA and GMM model? Briefly explain your thoughts.\n",
    "3. What are the disadvantages of pLSA? Consider its generalizing ability to new unseen document and its parameter complexity, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers**\n",
    "\n",
    "<span style=\"color:blue\"> Type your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Questions (10 points): LDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned document and topic modeling techiques. As mentioned in the lecture, most frequently used topic models are pLSA and LDA. [Latent Dirichlet allocation (LDA)](https://ai.stanford.edu/~ang/papers/nips01-lda) proposed by David M. Blei, Andrew Y. Ng, and Michael I. Jordan, posits that each document is generated as a mixture of topics where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable.\n",
    "\n",
    "In this question, please read the paper and/or tutorials of LDA and finish the following questions and tasks:\n",
    "\n",
    "(1) What are the differences between pLSA and LDA? List at least one advantage of LDA over pLSA? \n",
    "\n",
    "(2) Show a demo of LDA with brief result analysis on any corpus and discuss what real-world applications can be supported by LDA. Note: You do not need to implement LDA algorithms from scratch. You may use multiple packages such as `nltk`, `gensim`, `pyLDAvis` (added on the `cs145hw6.yml`) to help show the demo within couple of lines of code. If you'd like to use other packages, feel free to install them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers**\n",
    "\n",
    "<span style=\"color:blue\"> Type your answer here! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Homework 6 :)\n",
    "Please printout the Jupyter notebook and relevant code files that you work on and submit only 1 PDF file on GradeScope with page assigned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
