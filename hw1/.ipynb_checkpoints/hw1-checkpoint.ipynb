{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 1 \n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW1 is due on **11:59 PM PT, Oct 19 (Monday, Week 3)**. Please submit through GradeScope (you will receive an invite to Gradescope for CS145 Fall 2020.). \n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Wenxuan Liu, UID: 805152602** </span>\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW1 conda environment by the given `cs145hw1.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw1.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "OR \n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw1.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys \n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression \n",
    "This workbook will walk you through a linear regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 100)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1000, 100) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Closed form solution\n",
    "In this section, complete the `getBeta` function in `linear_regression.py` which use the close for solution of $\\hat{\\beta}$.\n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "Training error is:  0.08693886675396784\n",
      "Testing error is:  0.11017540281675804\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "beta = lm.train('0')\n",
    "y_train_pred = lm.predict(lm.train_x, beta)\n",
    "y_test_pred = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_error = lm.compute_mse(y_train_pred, lm.train_y)\n",
    "testing_error = lm.compute_mse(y_test_pred, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Batch gradient descent\n",
    "In this section, complete the `getBetaBatchGradient` function in `linear_regression.py` which compute the gradient of the objective fuction. \n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  1\n",
      "Training accuracy is:  0.08693897242330627\n",
      "Testing accuracy is:  0.11018612687974925\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "beta = lm.train('1')\n",
    "y_train_pred = lm.predict(lm.train_x, beta)\n",
    "y_test_pred = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_error = lm.compute_mse(y_train_pred, lm.train_y)\n",
    "testing_error = lm.compute_mse(y_test_pred, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_error)\n",
    "print('Testing accuracy is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stochastic gadient descent \n",
    "In this section, complete the `getBetaStochasticGradient` function in `linear_regression.py`, which use an estimated gradient of the objective function.\n",
    "\n",
    "Train you model by using `lm.train('2')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  2\n",
      "Training accuracy is:  0.09583964263625265\n",
      "Testing accuracy is:  0.11707460131538608\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.lr = 0.0005  # to avoid overflow\n",
    "beta = lm.train('2')\n",
    "y_train_pred = lm.predict(lm.train_x, beta)\n",
    "y_test_pred = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_error = lm.compute_mse(y_train_pred, lm.train_y)\n",
    "testing_error = lm.compute_mse(y_test_pred, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_error)\n",
    "print('Testing accuracy is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the MSE on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Apply z-score normalization for eachh featrure and comment whether or not it affect the three algorithm. \n",
    "3. Ridge regression is adding an L2 regularization term to the original objective function of mean squared error. The objective function become following: \n",
    "    $$ J(\\beta) = \\frac{1}{2n} \\sum_i \\left(x_i^T\\beta - y_i \\right)^2 + \\frac{\\lambda}{2n} \\sum_j \\beta_j^2 ,$$ \n",
    "where $\\lambda \\geq 0$, which is a hyper parameter that controls the trade off. Take the derivative of this provided objective function and derive the closed form solution for $\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "1. Compute the MSE on test dataset for each version:\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "Testing error is:  0.11017540281675804 \n",
      "\n",
      "Learning Algorithm Type:  1\n",
      "Testing error is:  0.1139244819746439 \n",
      "\n",
      "Learning Algorithm Type:  2\n",
      "Testing error is:  0.1973558594621252 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# training_error= 0\n",
    "testing_error= 0\n",
    "lm.lr = 0.002  # to avoid overflow\n",
    "for method in ('0','1','2'):\n",
    "    beta = lm.train(method)\n",
    "    testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "    print('Testing error is: ', testing_error, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "The MSE for each version of linear regression is not the same, but they are very close.\n",
    "\n",
    "They are different because they took different optimization paths such that '0' uses direct computation, '1' uses batch gradient descent, and '2' uses Stochastic gradient descent. They are very close because they are all optimized in the linear regression way and took the same learning rate, so the optimizing paths are very close.\n",
    "\n",
    "2. Test the normalized version of algorithms:\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "Testing error is:  0.11017540281675804 \n",
      "\n",
      "Learning Algorithm Type:  1\n",
      "Testing error is:  1.0811155806695387 \n",
      "\n",
      "Learning Algorithm Type:  2\n",
      "Testing error is:  0.11419008740447903 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# training_error= 0\n",
    "testing_error= 0\n",
    "lm.lr = 0.002  # to avoid overflow\n",
    "lm.normalize()\n",
    "for method in ('0','1','2'):\n",
    "    beta = lm.train(method)\n",
    "    testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "    print('Testing error is: ', testing_error, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "The z-score normalization for each feature does not affect the closed form, but would affect SGD and BGD. Because linear regression takes the form of multiplying $\\beta_i$ for each feature, so normalizing the feature basically would only affect the scale of the $\\beta_i$, but not affect the accuracy of the closed form.\n",
    "However, normalization affects gradient descent algorithms because it makes each feature equally important for optimization and it prevents quick converging for the gradient descent.\n",
    "\n",
    "3.\n",
    "The answer is $\\beta = (X^TX+\\lambda I)^{-1} X^T y$.\n",
    "The deduction steps are as followed:\n",
    "\n",
    "First, transform into matrix form:\n",
    "\n",
    "$$ J(\\beta)=\\frac{1}{2n}(X\\beta-Y)^T(X\\beta-Y)+\\frac{\\lambda}{2n}\\beta^T\\beta $$\n",
    "\n",
    "Then take the derivative(as we already know the left part which is the same as OLS):\n",
    "\n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta}=(X^TX\\beta - X^Ty)/n+\\frac{\\partial J}{\\partial \\beta}(\\frac{\\lambda}{2n}\\beta^T\\beta)$$\n",
    "\n",
    "$$ \\rightarrow \\frac{\\partial J(\\beta)}{\\partial \\beta} = (X^TX\\beta-X^Ty)/n+\\frac{\\lambda}{n}\\beta $$\n",
    "\n",
    "Set the derivative to zero:\n",
    "\n",
    "$$ 0=\\frac{1}{n}(X^TX\\beta-X^Ty+\\lambda\\beta) $$\n",
    "\n",
    "$$ \\rightarrow X^Ty = X^TX\\beta+\\lambda \\beta $$\n",
    "\n",
    "$$ \\rightarrow X^Ty = \\beta(X^TX+\\lambda I) $$\n",
    "\n",
    "$$ \\rightarrow \\beta = (X^TX+\\lambda I)^{-1} X^T y $$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic regression \n",
    "This workbook will walk you through a logistic regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 5)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "# As a sanity chech, we print out the size of the training data (1000, 5) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Batch gradiend descent\n",
    "In this section, complete the `getBeta_BatchGradient` in `logistic_regression.py`, which compute the gradient of the log likelihoood function. \n",
    "\n",
    "Complete the `compute_avglogL` function in `logistic_regression.py` for sanity check. \n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "And print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.5069864418045587 \t\n",
      "average logL for iteration 1000: -0.460100375350853 \t\n",
      "average logL for iteration 2000: -0.460100375350853 \t\n",
      "average logL for iteration 3000: -0.460100375350853 \t\n",
      "average logL for iteration 4000: -0.460100375350853 \t\n",
      "average logL for iteration 5000: -0.460100375350853 \t\n",
      "average logL for iteration 6000: -0.460100375350853 \t\n",
      "average logL for iteration 7000: -0.460100375350853 \t\n",
      "average logL for iteration 8000: -0.460100375350853 \t\n",
      "average logL for iteration 9000: -0.460100375350853 \t\n",
      "Training avgLogL:  -0.460100375350853\n",
      "Training accuracy is:  0.797\n",
      "Testing accuracy is:  0.7534791252485089\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.normalize()\n",
    "\n",
    "beta = lm.train('0')\n",
    "y_train_pred = lm.predict(lm.train_x, beta)\n",
    "y_test_pred = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_accuracy = lm.compute_accuracy(y_train_pred, lm.train_y)\n",
    "testing_accuracy = lm.compute_accuracy(y_test_pred, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Newton Raphhson\n",
    "In this section, complete the `getBeta_Newton` in `logistic_regression.py`, which make use of both first and second derivative.\n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.49788520389331575 \t\n",
      "average logL for iteration 500: -0.460100375350853 \t\n",
      "average logL for iteration 1000: -0.460100375350853 \t\n",
      "average logL for iteration 1500: -0.460100375350853 \t\n",
      "average logL for iteration 2000: -0.460100375350853 \t\n",
      "average logL for iteration 2500: -0.460100375350853 \t\n",
      "average logL for iteration 3000: -0.460100375350853 \t\n",
      "average logL for iteration 3500: -0.460100375350853 \t\n",
      "average logL for iteration 4000: -0.460100375350853 \t\n",
      "average logL for iteration 4500: -0.460100375350853 \t\n",
      "average logL for iteration 5000: -0.4601003753508531 \t\n",
      "average logL for iteration 5500: -0.460100375350853 \t\n",
      "average logL for iteration 6000: -0.460100375350853 \t\n",
      "average logL for iteration 6500: -0.460100375350853 \t\n",
      "average logL for iteration 7000: -0.460100375350853 \t\n",
      "average logL for iteration 7500: -0.460100375350853 \t\n",
      "average logL for iteration 8000: -0.460100375350853 \t\n",
      "average logL for iteration 8500: -0.460100375350853 \t\n",
      "average logL for iteration 9000: -0.460100375350853 \t\n",
      "average logL for iteration 9500: -0.460100375350853 \t\n",
      "Training avgLogL:  -0.460100375350853\n",
      "Training accuracy is:  0.797\n",
      "Testing accuracy is:  0.7534791252485089\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.normalize()\n",
    "\n",
    "beta = lm.train('1')\n",
    "y_train_pred = lm.predict(lm.train_x, beta)\n",
    "y_test_pred = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_accuracy = lm.compute_accuracy(y_train_pred, lm.train_y)\n",
    "testing_accuracy = lm.compute_accuracy(y_test_pred, lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the accuracy on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Regularization. Similar to linear regression, an regularization term could be added to logistic regression. \n",
    "The objective function becomes following: \n",
    "    $$ J(\\beta) = -\\frac{1}{n} \\sum_i \\left(y_i x_i^T \\beta - \\log \\left( 1+ \\exp\\{ x_i^T \\beta \\} \\right) \\right) + \\lambda \\sum_j \\beta_j^2,$$ \n",
    "where $\\lambda \\geq 0$, which is a hyper parameter that controls the trade off. Take the derivative $\\frac{\\partial J(\\beta)}{\\partial \\beta_j}$ of this provided objective function and provide the batch gradient descent update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "1. Their accuracy are same(around 0.75). Because the two algorithms both converge to the optimized point, although with different converging speed. However, with large enough iterations, they basically took the similar regression path, and reach the same optimized accuracy.\n",
    "\n",
    "2. The result is $\\beta_j^{t+1} = B_j^t + \\eta \\left(\\frac{1}{n} \\sum_{i=1}^n x_{ij}(y_i-p_i(\\beta)) - 2 \\lambda \\beta_j\\right)$\n",
    "\n",
    "First, compute the gradient(as we already know the first derivative of the left part)\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\beta_j} = -\\frac{1}{n} \\sum_{i=1}^n x_{ij}(y_i-p_i(\\beta)) + \\frac{\\partial}{\\partial \\beta_j}\\lambda \\sum_j \\beta_j^2$$\n",
    "\n",
    "$$ \\rightarrow \\frac{\\partial J}{\\partial \\beta_j} = -\\frac{1}{n} \\sum_{i=1}^n x_{ij}(y_i-p_i(\\beta)) + 2 \\lambda \\beta_j$$\n",
    "\n",
    "Then, as we have the gradient ascent update $$ \\beta_j^{t+1} = B_j^t - \\eta \\frac{\\partial J}{\\partial \\beta_j} $$, we can further do the deduction:\n",
    "\n",
    "$$ \\beta_j^{t+1} = B_j^t + \\eta \\left(\\frac{1}{n} \\sum_{i=1}^n x_{ij}(y_i-p_i(\\beta)) - 2 \\lambda \\beta_j\\right) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize the decision boundary on a toy dataset\n",
    "\n",
    "In this subsection, you will use the same implementation for another small dataset with each datapoint $x$ with only two features $(x_1, x_2)$ to visualize the decision boundary of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (99, 2)\n",
      "Training labels shape: (99,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression(verbose = False)\n",
    "lm.load_data('./data/logistic-regression-toy.csv','./data/logistic-regression-toy.csv')\n",
    "# As a sanity chech, we print out the size of the training data (99,2) and training labels (99,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, you can apply the same implementation of logistic regression model (either in 2.1 or 2.2) to the toy dataset. Print out the $\\hat{\\beta}$ after training and accuracy on the train set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training avgLogL:  -0.32914743129571195\n",
      "Beta is [-0.04717577  1.46005896  2.06586134]\n",
      "Training accuracy is:  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "training_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('0')\n",
    "\n",
    "y_train_pred = lm.predict(lm.train_x, beta)\n",
    "y_test_pred = lm.predict(lm.test_x, beta)\n",
    "\n",
    "training_accuracy = lm.compute_accuracy(y_train_pred, lm.train_y)\n",
    "testing_accuracy = lm.compute_accuracy(y_test_pred, lm.test_y)\n",
    "\n",
    "print('Beta is', beta)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to plot the decision boundary of your learned logistic regression classifier. Generally, a decision boundary is the region of a space in which the output label of a classifier is ambiguous. That is, in the given toy data, given a datapoint $x=(x_1, x_2)$ on the decision boundary, the logistic regression classifier cannot decide whether $y=0$ or $y=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Is the decision boundary for logistic regression linear? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the decision boundary in the following cell. Note that the code to plot the raw data points are given. You may need `plt.plot` function (see [here](https://matplotlib.org/tutorials/introductory/pyplot.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot the raw data\n",
    "df = pd.concat([lm.train_x, lm.train_y], axis=1)\n",
    "groups = df.groupby(\"y\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[\"x1\"], group[\"x2\"], marker=\"o\", linestyle=\"\", label=name)\n",
    "    \n",
    "# plot the decision boundary on top of the scattered points\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "x = np.array([-2, 2])\n",
    "y = (beta[0] + beta[1]*x)/(-beta[2])\n",
    "\n",
    "plt.plot(x, y)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 1 :)\n",
    "After you've finished the homework, please print out the entire `ipynb` notebook and two `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
